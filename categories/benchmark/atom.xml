<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Category: Benchmark | Alessandro Nadalin]]></title>
  <link href="https://odino.org/categories/benchmark/atom.xml" rel="self"/>
  <link href="https://odino.org/"/>
  <updated>2022-11-18T09:33:53+00:00</updated>
  <id>https://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[(Equal) Web Application Performance]]></title>
    <link href="https://odino.org/equal-web-application-performance/"/>
    <updated>2018-12-07T18:30:00+00:00</updated>
    <id>https://odino.org/equal-web-application-performance</id>
    <content type="html"><![CDATA[<p>The idea behind equal web performance is that you should
<strong>stop looking at metrics other than the 99th percentile</strong>:
resources shoud be &ldquo;equally&rdquo; distributed across all of your clients so
that a few clients don&rsquo;t act as a bottleneck for the others.</p>

<!-- more -->


<p>Note that during outages, or when you run at &ldquo;degraded&rdquo; performance,
this is an acceptable practice instead &mdash; drop some of the traffic to
make sure a good portion of your users are able to use
the service, rather than trying to serve more users with
nefarious consequences.</p>

<h2>Averages</h2>

<p>When you measure performances, one of the worst mistakes you
can do is to have a look at averages, as they really don&rsquo;t
tell you anything about how your app is performing.</p>

<p>Don&rsquo;t believe me?
Let&rsquo;s consider this: I have an application with an average
response time of ~250ms, over 4 requests. The individual requests
recorded these times:</p>

<ul>
<li>request A took 5ms</li>
<li>request B took 5ms</li>
<li>request C took 5ms</li>
<li>request D took 1000ms</li>
</ul>


<p>As you can see, request D will most likely result in an
unhappy client, one that has to wait around a second to
get a response from our API. If we only look at the average,
though, we wouldn&rsquo;t notice this problem, as ~250ms looks
like an acceptable number to us.</p>

<p>Averages will simply give you a rough idea (which, to me, is fairly
useless), but won&rsquo;t be able to tell you whether you&rsquo;re prioritizing some clients over others, or whether your performance are at an acceptable
level for all of your clients &mdash; this is why you need to start
looking at p50 and p99.</p>

<h2>p50</h2>

<p>The p50 (otherwise known as median), like the average, is kind of an illusion to me: it tells you that half of your traffic is served within a
particular timeframe.</p>

<p><img class="center" src="/images/p50-nr.png"></p>

<p>Even though it is definitely acceptable to look at the p50, I still
feel this is a very &ldquo;elitist&rdquo; metric: what I&rsquo;m really after is making
it great for everyone in the audience, not just half of them.</p>

<h2>p99</h2>

<p>The 99th percentile is, in my head, kind of like the ultimate metric:
I will define a certain number in my head (say 200ms) and think that
&ldquo;<em>I&rsquo;d be extremely happy if all my traffic was served within that
timeframe</em>&rdquo; &mdash; that&rsquo;s all I look at.</p>

<p><img class="center" src="/images/p99-nr-overview.png"></p>

<p>I might sometime get distracted by the p50, but the p99 is
really all that matters to me: if the p50 grows from 25 to
100ms I won&rsquo;t be bothered as much, but if the p99 increases
from 100 to 200ms I&rsquo;d be worried sick about it, as I know it
has a larger impact, while the p50&rsquo;s variation is probably
barely noticeable by our clients.</p>

<h2>p100</h2>

<p>Is a dream: systems fail, networks are unreliable, bugs happen.
Consider the p100 utopia, and keep looking at the p99
instead.</p>

<h2>Optimizing for the p99</h2>

<p>I also generally found out that when you optimize for the p99,
you really end up optimizing for everyone: my suggestion is to
look at the slowest responses you&rsquo;re serving and to tackle those
first. Since they are the slowest, fixing these will improve your
averages as well as help all other transactions execute faster,
by removing a slow one that might hog CPU, or memory.</p>

<p>Here are a couple examples of optimizing for the p99 from
our work at <a href="https://tech.namshi.io">Namshi</a> &mdash; the first is taken
from our RDS metrics, and it&rsquo;s the p99 CPU utilization after optimizing
the top queries in our slow query log:</p>

<p><img class="center" src="/images/p99-cw.png"></p>

<p>The second one is from NewRelic, and it shows that by removing
k8s resource limits (as advised by <a href="https://www.slideshare.net/try_except_/optimizing-kubernetes-resource-requestslimits-for-costefficiency-and-latency-highload/42">Zalando</a>) we made apps a lot
more reliable (this is super-duper counter-intuitive, I know):</p>

<p><img class="center" src="/images/p99-newrelic.png"></p>

<p>Adios!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Benchmarking JavaScript Snippets]]></title>
    <link href="https://odino.org/benchmarking-javascript-snippets/"/>
    <updated>2017-04-12T18:32:00+00:00</updated>
    <id>https://odino.org/benchmarking-javascript-snippets</id>
    <content type="html"><![CDATA[<p>A few days back I was playing around with
<a href="https://lodash.com/">lodash</a> to figure out if
some of its functions would <a href="/beware-of-lodash-and-the-cost-of-abstractions/">add significant overhead</a>
as opposed to their vanilla counterparts: in doing
so I discovered <a href="https://github.com/logicalparadox/matcha">matcha</a>,
an amazing tool for benchmarking JS code.</p>

<!-- more -->


<p>Matcha comes with a clear goal in mind: &ldquo;no waste of time, show me the code&rdquo;, as
it&rsquo;s incredibly easy to setup and start benchmarking.</p>

<p>We first need to create
a benchmark file (say <code>index.js</code>) and start adding test suites; a suite simply
describes the <em>functionality</em> we want to benchmark, such as
creating a new object, handpicking properties, from a &ldquo;larger&rdquo; one:</p>

<p>``` js
let _ = require(&lsquo;lodash&rsquo;)</p>

<p>suite(&lsquo;picking properties from an object&rsquo;, function () {
  // &hellip;
});
```</p>

<p>At this point, we can create a custom object that&rsquo;s going to be used by the
implementations we want to benchmark:</p>

<p>``` js
let _ = require(&lsquo;lodash&rsquo;)</p>

<p>suite(&lsquo;picking properties from an object&rsquo;, function () {
  let obj = {</p>

<pre><code>name: 'alex',
age: 28,
hair: 'enough',
status: 'married',
job: 'who really knows',
</code></pre>

<p>  }
});
```</p>

<p>and, still inside the suite, start adding our implementations:</p>

<p>``` js
let _ = require(&lsquo;lodash&rsquo;)</p>

<p>suite(&lsquo;picking properties from an object&rsquo;, function () {
  let obj = {</p>

<pre><code>name: 'alex',
age: 28,
hair: 'enough',
status: 'married',
job: 'who really knows',
</code></pre>

<p>  }</p>

<p>  bench(&lsquo;lodash _.pick&rsquo;, function() {</p>

<pre><code>return _.pick(obj, ['name', 'age'])
</code></pre>

<p>  });</p>

<p>  bench(&lsquo;vanilla&rsquo;, function() {</p>

<pre><code>return {
  name: obj.name,
  age: obj.age,
}
</code></pre>

<p>  });
});
```</p>

<p>Assuming matcha is installed globally (<code>npm install -g matcha</code>) you can then
simply:</p>

<p>``` bash
$ matcha index.js</p>

<pre><code>                  picking properties from an object
     651,233 op/s » lodash _.pick
  80,885,471 op/s » vanilla
</code></pre>

<p>```</p>

<p>As you see, matcha starts executing those functions repeatedly, for a few seconds,
and outputs how many executions it was able to do within that timeframe.</p>

<p>The matcha bin will autorun tests as soon as you place under a <code>./benchmarks</code>
folder, and you can customize the number of iterations to run the snippets
for:</p>

<p>``` js
suite(&lsquo;picking properties from an object&rsquo;, function () {
  set(&lsquo;iterations&rsquo;, 10000);</p>

<p>  // &hellip;
})
```</p>

<p>Support for async benchmarks is provided with a <code>next</code> callback that you&rsquo;ll need to
call once the async operation is over:</p>

<p>``` js
bench(&lsquo;some async thingy with promises&rsquo;, function(next) {
  somePromise(someParams).then(next)
});</p>

<p>bench(&lsquo;some async thingy with goold old callbacks&rsquo;, function(next) {
  someFn(someParams, next)
});
```</p>

<p>Can it get any easier?</p>
]]></content>
  </entry>
  
</feed>
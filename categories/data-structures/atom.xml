<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Category: Data Structures | Alessandro Nadalin]]></title>
  <link href="https://odino.org/categories/data-structures/atom.xml" rel="self"/>
  <link href="https://odino.org/"/>
  <updated>2021-07-18T10:30:01+00:00</updated>
  <id>https://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[This Is How a (Dumb) Hash Table Works]]></title>
    <link href="https://odino.org/this-is-how-a-dumb-hashtable-works/"/>
    <updated>2018-04-04T05:27:00+00:00</updated>
    <id>https://odino.org/this-is-how-a-dumb-hashtable-works</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/hashtable.png"></p>

<p>How beautiful is <code>{}</code>?</p>

<p>It lets you store values by key, and retrieve them in a very cost-efficient manner
(<code>O(1)</code>, more on this later).</p>

<p>In this post I want to implement a very basic hash table, and have a look at its inner
workings to explain one of the most ingenious ideas in computer science.</p>

<!-- more -->


<h2>The problem</h2>

<p>Imagine you&rsquo;re building a new programming language: you start by having pretty
simple types (strings, integers, floats, &hellip;) and then proceed to implement very basic
data structures &mdash; first you come up with the array (<code>[]</code>), then comes the hash table
(otherwise known as dictionary, associative array, hashmap, map and&hellip;the list goes on).</p>

<p>Ever wondered how they work? How they&rsquo;re so damn fast?</p>

<p>Well, let&rsquo;s say that JavaScript did not have have <code>{}</code> or <code>new Map()</code>, and let&rsquo;s
implement our very own <code>DumbMap</code>!</p>

<h2>A note on complexity</h2>

<p>Before we get the ball rolling, we need to understand how complexity of a function works:
Wikipedia has a good refresher on <a href="https://en.wikipedia.org/wiki/Computational_complexity_theory">computational complexity</a>,
but I&rsquo;ll add a brief explanation for the lazy ones.</p>

<p>Complexity measures how many steps are required by our function &mdash; the fewer steps,
the faster the execution (also known as &ldquo;running time&rdquo;).</p>

<p>Let&rsquo;s a look at the following snippet:</p>

<p><code>js
function fn(n, m) {
  return n * m
}
</code></p>

<p>The computational complexity (from now simply &ldquo;complexity&rdquo;) of <code>fn</code> is <code>O(1)</code>,
meaning that it&rsquo;s constant (you can read <code>O(1)</code> as &ldquo;<em>the cost is one</em>&rdquo;): no matter
what arguments you pass, the platform that runs this code only has to do one
operation (multiply <code>n</code> into <code>m</code>). Again, since it&rsquo;s one operation, the cost is
referred as <code>O(1)</code>.</p>

<p>Complexity is measured by assuming arguments of your function could have very large values.
Let&rsquo;s look at this example:</p>

<p>``` js
function fn(n, m) {
  let s = 0</p>

<p>  for (i = 0; i &lt; 3; i++) {</p>

<pre><code>s += n * m
</code></pre>

<p>  }</p>

<p>  return s
}
```</p>

<p>You would think its complexity is <code>O(3)</code>, right?</p>

<p>Again, since complexity is measured in the context of very large arguments,
we tend to &ldquo;drop&rdquo; constants and consider <code>O(3)</code> the same as <code>O(1)</code>. So, even in this case, we would say that the complexity of
<code>fn</code> is <code>O(1)</code>. No matter what the value of <code>n</code> and <code>m</code> are, you always end up
doing 3 operations &mdash; which, again, is a constant cost (therefore <code>O(1)</code>).</p>

<p>Now this example is a little bit different:</p>

<p>``` js
function fn(n, m) {
  let s = []</p>

<p>  for (i = 0; i &lt; n; i++) {</p>

<pre><code>s.push(m)
</code></pre>

<p>  }</p>

<p>  return s
}
```</p>

<p>As you see, we&rsquo;re looping as many times as the value of <code>n</code>, which could be in the
millions. In this case we define the complexity of this function as <code>O(n)</code>, as you
will need to do as many operations as the value of one of your arguments.</p>

<p>Other examples?</p>

<p>``` js
function fn(n, m) {
  let s = []</p>

<p>  for (i = 0; i &lt; 2 * n; i++) {</p>

<pre><code>s.push(m)
</code></pre>

<p>  }</p>

<p>  return s
}
```</p>

<p>This examples loops <code>2 * n</code> times, meaning the complexity should be <code>O(2n)</code>.
Since we mentioned that constants are &ldquo;ignored&rdquo; when calculating the complexity
of a function, this example is also classified as <code>O(n)</code>.</p>

<p>One more?</p>

<p>``` js
function fn(n, m) {
  let s = []</p>

<p>  for (i = 0; i &lt; n; i++) {</p>

<pre><code>for (i = 0; i &lt; n; i++) {
  s.push(m)
}
</code></pre>

<p>  }</p>

<p>  return s
}
```</p>

<p>Here we are looping over <code>n</code> and looping again inside the main loop, meaning the
complexity is &ldquo;squared&rdquo; (<code>n * n</code>): if <code>n</code> is 2, we will run <code>s.push(m)</code> 4 times,
if 3 we will run it 9 times, and so on.</p>

<p>In this case, the complexity of the function is referred as <code>O(nÂ²)</code>.</p>

<p>One last example?</p>

<p>``` js
function fn(n, m) {
  let s = []</p>

<p>  for (i = 0; i &lt; n; i++) {</p>

<pre><code>s.push(n)
</code></pre>

<p>  }</p>

<p>  for (i = 0; i &lt; m; i++) {</p>

<pre><code>s.push(m)
</code></pre>

<p>  }</p>

<p>  return s
}
```</p>

<p>In this case we don&rsquo;t have nested loops, but we loop twice over two different arguments:
the complexity is defined as <code>O(n+m)</code>. Crystal clear.</p>

<p>Now that you&rsquo;ve just got a brief introduction (or refresher) on complexity, it&rsquo;s
very easy to understand that a function with complexity <code>O(1)</code> is going to perform
much better than one with <code>O(n)</code>.</p>

<p>Hash tables have a <code>O(1)</code> complexity<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>: in layman&rsquo;s terms, they&rsquo;re <strong>superfast</strong>.
Let&rsquo;s move on.</p>

<h2>Let&rsquo;s build a (dumb) hash table</h2>

<p>Our hash table has 2 simple methods &mdash; <code>set(x, y)</code> and <code>get(x)</code>. Let&rsquo;s start writing
some code:</p>

<p>``` js
class DumbMap {
  get(x) {</p>

<pre><code>console.log(`get ${x}`)
</code></pre>

<p>  }</p>

<p>  set(x, y) {</p>

<pre><code>console.log(`set ${x} to ${y}`)
</code></pre>

<p>  }
}</p>

<p>let m = new DumbMap()</p>

<p>m.set(&lsquo;a&rsquo;, 1) // &ldquo;set a to 1&rdquo;
m.get(&lsquo;a&rsquo;) // &ldquo;get a&rdquo;
```</p>

<p>ans let&rsquo;s implement a very simple, inefficient way to store these key-value pairs
and retrieve them later on. We first start by storing them in an internal array
(remember, we can&rsquo;t use <code>{}</code> since we are implementing <code>{}</code> &mdash; mindblown!):</p>

<p>``` js
class DumbMap {
  constructor() {</p>

<pre><code>this.list = []
</code></pre>

<p>  }</p>

<p>  &hellip;</p>

<p>  set(x, y) {</p>

<pre><code>this.list.push([x, y])
</code></pre>

<p>  }
}
```</p>

<p>then it&rsquo;s simply a matter of getting the right element from the list:</p>

<p>``` js
get(x) {
  let result</p>

<p>  this.list.forEach(pairs => {</p>

<pre><code>if (pairs[0] === x) {
  result = pairs[1]
}
</code></pre>

<p>  })</p>

<p>  return result
}
```</p>

<p>Our full example:</p>

<p>``` js
class DumbMap {
  constructor() {</p>

<pre><code>this.list = []
</code></pre>

<p>  }</p>

<p>  get(x) {</p>

<pre><code>let result

this.list.forEach(pairs =&gt; {
  if (pairs[0] === x) {
    result = pairs[1]
  }
})

return result
</code></pre>

<p>  }</p>

<p>  set(x, y) {</p>

<pre><code>this.list.push([x, y])
</code></pre>

<p>  }
}</p>

<p>let m = new DumbMap()</p>

<p>m.set(&lsquo;a&rsquo;, 1)
console.log(m.get(&lsquo;a&rsquo;)) // 1
console.log(m.get(&lsquo;I_DONT_EXIST&rsquo;)) // undefined
```</p>

<p>Our <code>DumbMap is amazing</code>! It works right out of the box, but how will it perform when we add a large amount of key-value
pairs?</p>

<p>Let&rsquo;s try a simple benchmark &mdash; we will first try to find a non-existing element
in an hash table with very few elements, and then try the same in one with a large quantity
of elements:</p>

<p>``` js
let m = new DumbMap()
m.set(&lsquo;x&rsquo;, 1)
m.set(&lsquo;y&rsquo;, 2)</p>

<p>console.time(&lsquo;with very few records in the map&rsquo;)
m.get(&lsquo;I_DONT_EXIST&rsquo;)
console.timeEnd(&lsquo;with very few records in the map&rsquo;)</p>

<p>m = new DumbMap()</p>

<p>for (x = 0; x &lt; 1000000; x++) {
  m.set(<code>element${x}</code>, x)
}</p>

<p>console.time(&lsquo;with lots of records in the map&rsquo;)
m.get(&lsquo;I_DONT_EXIST&rsquo;)
console.timeEnd(&lsquo;with lots of records in the map&rsquo;)
```</p>

<p>The results? Not so encouraging:</p>

<p><code>bash
with very few records in the map: 0.118ms
with lots of records in the map: 14.412ms
</code></p>

<p>In our implementation, we need to loop through all the elements inside <code>this.list</code>
in order to find one with the matching key. The cost is <code>O(n)</code>, and it&rsquo;s quite
terrible.</p>

<h2>Make it fast(er)</h2>

<p>We need to find a way to avoid looping through our list: time to put <em>hash</em>
back into the <em>hash table</em>.</p>

<p>Ever wondered why this data structure is called <strong>hash</strong> table? That&rsquo;s because
a hashing function is used on the keys that you set and get: we will use this
function to turn our key into an integer <code>i</code>, and store our value at index <code>i</code>
of our internal list. Since accessing an element, by its index, from a list has
a constant cost (<code>O(1)</code>), then the hash table will also have a cost of <code>O(1)</code>.</p>

<p>Let&rsquo;s try this out:</p>

<p>``` js
let hash = require(&lsquo;string-hash&rsquo;)</p>

<p>class DumbMap {
  constructor() {</p>

<pre><code>this.list = []
</code></pre>

<p>  }</p>

<p>  get(x) {</p>

<pre><code>return this.list[hash(x)]
</code></pre>

<p>  }</p>

<p>  set(x, y) {</p>

<pre><code>this.list[hash(x)] = y
</code></pre>

<p>  }
}
```</p>

<p>Here we are using the <a href="https://www.npmjs.com/package/string-hash">string-hash</a>
module which simply converts a string to a numeric hash, and use it to store
and fetch elements at index <code>hash(key)</code> of our list. The results?</p>

<p><code>bash
with lots of records in the map: 0.013ms
</code></p>

<p>W &ndash; O &ndash; W. This is what I&rsquo;m talking about!</p>

<p>We don&rsquo;t have to loop through all elements in the list and retrieving elements
from <code>DumbMap</code> is fast as hell!</p>

<p>Let me put this as straightforward as possible: <strong>hashing is what makes hash tables
extremely efficient</strong>. No magic. Nothing more. Nada. Just a simple, clever, ingenious
idea.</p>

<h2>The cost of picking the right hashing function</h2>

<p>Of course, <strong>picking a fast hashing function is very important</strong>: if our <code>hash(key)</code>
runs in a few seconds, our function will be quite slow regardless of its complexity.</p>

<p>At the same time, <strong>it&rsquo;s very important to make sure that our hashing function doesn&rsquo;t
produce a lot of collisions</strong>, as they would be detrimental to the complexity of our
hash table.</p>

<p>Confused? Let&rsquo;s take a closer look at collisions.</p>

<h2>Collisions</h2>

<p>You might think &ldquo;<em>Ah, a good hashing function never generates collisions!</em>&rdquo;: well,
come back to the real world and think again. <a href="https://security.googleblog.com/2017/02/announcing-first-sha1-collision.html">Google was able to produce collisions
for the SHA-1 hashing algorithm</a>,
and it&rsquo;s just a matter of time, or computational power, before a hashing function
cracks and returns the same hash for 2 different inputs. Always assume your hashing
function generates collisions and implement the right defense against such cases.</p>

<p>Case in point, let&rsquo;s try to use a <code>hash()</code> function that generates a lot of collisions:</p>

<p>``` js
function divide(int) {
  int = Math.round(int / 2)</p>

<p>  if (int > 10) {</p>

<pre><code>return divide(int)
</code></pre>

<p>  }</p>

<p>  return int
}</p>

<p>function hash(key) {
  let h = require(&lsquo;string-hash&rsquo;)(key)
  return divide(h)
}
```</p>

<p>This function uses an array of 10 elements to store values, meaning that elements
are likely to be replaced &mdash; a nasty bug in our <code>DumbMap</code>:</p>

<p>``` js
let m = new DumbMap()</p>

<p>for (x = 0; x &lt; 1000000; x++) {
  m.set(<code>element${x}</code>, x)
}</p>

<p>console.log(m.get(&lsquo;element0&rsquo;)) // 999988
console.log(m.get(&lsquo;element1&rsquo;)) // 999988
console.log(m.get(&lsquo;element1000&rsquo;)) // 999987
```</p>

<p>In order to resolve the issue, we can simply store multiple key-value pairs at the
same index &mdash; let&rsquo;s amend our hash table:</p>

<p>``` js
class DumbMap {
  constructor() {</p>

<pre><code>this.list = []
</code></pre>

<p>  }</p>

<p>  get(x) {</p>

<pre><code>let i = hash(x)

if (!this.list[i]) {
  return undefined
}

let result

this.list[i].forEach(pairs =&gt; {
  if (pairs[0] === x) {
    result = pairs[1]
  }
})

return result
</code></pre>

<p>  }</p>

<p>  set(x, y) {</p>

<pre><code>let i = hash(x)

if (!this.list[i]) {
  this.list[i] = []
}

this.list[i].push([x, y])
</code></pre>

<p>  }
}
```</p>

<p>As you might notice, here we fall back to our original implementation: store a list
of key-value pairs and loop through each of them, which is going to be quite slow
when there are a lot of collisions for a particular index of the list.</p>

<p>Let&rsquo;s benchmark this using our own <code>hash()</code> function that generates indexes from 1 to 10:</p>

<p><code>bash
with lots of records in the map: 11.919ms
</code></p>

<p>and by using the hash function from <code>string-hash</code>, which generates random indexes:</p>

<p><code>bash
with lots of records in the map: 0.014ms
</code></p>

<p>Whoa! There&rsquo;s the cost of picking the right hashing function &mdash; fast enough that
it doesn&rsquo;t slow our execution down on its own, and good enough that it doesn&rsquo;t produce a lot
of collisions.</p>

<h2>Generally O(1)</h2>

<p>Remember my words?</p>

<blockquote><p>Hash tables have a <code>O(1)</code> complexity</p></blockquote>

<p>Well, I lied: the complexity of an hash table depends on the hashing function you
pick. The more collisions you generate, the more the complexity tends toward <code>O(n)</code>.</p>

<p>A hashing function such as:</p>

<p><code>js
function hash(key) {
  return 0
}
</code></p>

<p>would mean that our hash table has a complexity of <code>O(n)</code>.</p>

<p>This is why, in general, computational complexity has 3 measures: best, average
and worst-case scenarios. Hash tables have a <code>O(1)</code> complexity in best and average case scenarios, but fall
to <code>O(n)</code> in their worst-case scenario.</p>

<p>Remember: <strong>a good hashing function is the key to an efficient hash table</strong> &mdash; nothing more, nothing less.</p>

<h2>More on collisions&hellip;</h2>

<p>The technique we used to fix <code>DumbMap</code> in case of collisions is called <a href="https://xlinux.nist.gov/dads/HTML/separateChaining.html">separate chaining</a>:
we store all the key-pairs that generate collisions in a list and loop through
them.</p>

<p>Another popular technique is <a href="https://en.wikipedia.org/wiki/Open_addressing">open addressing</a>:</p>

<ul>
<li>at each index of our list we store <strong>one and one only key-value pair</strong></li>
<li>when trying to store a pair at index <code>x</code>, if there&rsquo;s already a key-value pair, try to store our new pair at <code>x + 1</code></li>
<li>if <code>x + 1</code> is taken, try <code>x + 2</code> and so on&hellip;</li>
<li>when retrieving an element, hash the key and see if the element at that position (<code>x</code>) matches our key</li>
<li>if not, try to access the element at position <code>x + 1</code></li>
<li>rinse and repeat until you get to the end of the list, or when you find an empty index &mdash; that means our element is not in the hash table</li>
</ul>


<p>Smart, simple, elegant and <a href="http://cseweb.ucsd.edu/~kube/cls/100/Lectures/lec16/lec16-28.html">usually very efficient</a>!</p>

<h2>FAQs (or TL;DR)</h2>

<h4>Does a hash table hash the values we&rsquo;re storing?</h4>

<p>No, keys are hashed so that they can be turned into an integer <code>i</code>, and both keys
and values are stored at position <code>i</code> in a list.</p>

<h4>Do the hashing functions used by hash tables generate collisions?</h4>

<p>Absolutely &mdash; so hash tables are implemented with <a href="https://en.wikipedia.org/wiki/Hash_table#Collision_resolution">defense strategies</a>
to avoid nasty bugs.</p>

<h4>Do hash tables use a list or a linked list internally?</h4>

<p>It depends, <a href="https://stackoverflow.com/questions/13595767/why-do-hash%20tables-use-a-linked-list-over-an-array-for-the-bucket">both can work</a>.
In our examples, we use the JavaScript array (<code>[]</code>) that can be <a href="https://www.quora.com/Do-arrays-in-JavaScript-grow-dynamically">dynamically resized</a>:</p>

<p>``` js</p>

<blockquote><p>a = []</p>

<p>a[3] = 1</p>

<p>a
[ <3 empty items>, 1 ]
```</p></blockquote>

<h4>Why did you pick JavaScript for the examples? JS arrays ARE hash tables!</h4>

<p>For example:</p>

<p>``` js</p>

<blockquote><p> a = []
[]
a[&ldquo;some&rdquo;] = &ldquo;thing&rdquo;
&lsquo;thing&rsquo;
a
[ some: &lsquo;thing&rsquo; ]
typeof a
&lsquo;object&rsquo;
```</p></blockquote>

<p>I know, damn JavaScript.</p>

<p>JavaScript is &ldquo;universal&rdquo; and probably the easiest language to understand when looking
at some sample code. I agree JS might not be the best language, but I hope these
examples are clear enough.</p>

<h4>Is your example a really good implementation of an hash table? Is it really THAT simple?</h4>

<p>No, not at all.</p>

<p>Have a look at &ldquo;<a href="http://www.mattzeunert.com/2017/02/01/implementing-a-hash-table-in-javascript.html">implementing a hash table in JavaScript</a>&rdquo;
by <a href="http://www.mattzeunert.com/">Matt Zeunert</a>, as it will give you a bit more
context. There&rsquo;s a lot more to learn, so I would also suggest you to also have a look at:</p>

<ul>
<li><a href="http://cseweb.ucsd.edu/~kube/cls/100/Lectures/lec16/lec16.html">Paul Kube&rsquo;s course on hash tables</a></li>
<li><a href="https://www.geeksforgeeks.org/implementing-our-own-hash-table-with-separate-chaining-in-java/">Implementing our Own Hash Table with Separate Chaining in Java</a></li>
<li><a href="https://algs4.cs.princeton.edu/34hash/">Algorithms, 4th Edition &ndash; Hash tables</a></li>
<li><a href="http://www.ilikebigbits.com/blog/2016/8/28/designing-a-fast-hash-table">Designing a fast hash table</a></li>
</ul>


<h2>In the end&hellip;</h2>

<p>Hash tables are a very clever idea we use on a regular basis: no matter
whether you create a <a href="https://stackoverflow.com/questions/114830/is-a-python-dictionary-an-example-of-a-hash-table">dictionary in Python</a>, an <a href="https://stackoverflow.com/a/3134315/934439">associative array in PHP</a> or a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map">Map
in JavaScript</a> &mdash; they all share the same concepts and beautifully work to let us
store and retrieve element by an identifier, at a (most likely) constant cost.</p>

<p>Hope you enjoyed this article, and feel free to share your feedback with me.</p>

<p><em>A
special thanks goes to <a href="https://github.com/joejean">Joe</a> who helped me by reviewing
this article.</em></p>

<p>Adios!</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>I know, I know, I&rsquo;m kinda lying here. Read until the end of the post ;&ndash;) <a href='#fnref:1' rev='footnote'>â©</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Bloom Filters: When Data Structures Get Smart]]></title>
    <link href="https://odino.org/bloom-filters-when-data-structures-get-smart/"/>
    <updated>2018-02-10T21:09:00+00:00</updated>
    <id>https://odino.org/bloom-filters-when-data-structures-get-smart</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/membership.png"></p>

<p>Keeping up with my quest on exploring <a href="/categories/probabilistic-data-structures/">probabilistic data structures</a>,
today I am going to present you <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a>,
an ingenious idea that allows us to quickly verify membership within a set.</p>

<p>As usual, I sound awful when using grown-up words: filters? Membership? Verification?
Just bear with me for a few minutes as we&rsquo;re about to delve into one of the most
ingenious ideas in the world of computer science, one that was born almost 50
years ago.</p>

<!-- more -->


<h2>Who invited this guy?!?</h2>

<p>If you remember my recent <a href="/my-favorite-data-structure-hyperloglog/">post on HyperLogLog and the cardinality-estimation problem</a>,
there I introduced Tommy, a friend of mine that joins me at meetups and conferences.</p>

<p>Today Tommy and I are extremely busy trying to organize an invite-only hackathon
for the best developers in the area: we recently setup a website with sign ups capped at
100 people, and have just reached the venue with a printed list of people who have made it through.</p>

<p>Unfortunately, since we have limited space at the venue, we need to make sure that
each and every person who wants to come in signed up online and,
if not, we unfortunately have to turn them down. Armed with a list of names,
Tommy starts processing each and everyone while I take care of setting up the rooms
and so on:</p>

<p><blockquote><p>Hi there, I am Tommy! Did you by any chance sign up online? What is your name?</p></blockquote></p>

<p><blockquote><p>Yeah man, I definitely did! Why do you need my name?</p></blockquote></p>

<p><blockquote><p>Unfortunately we only have seats and food for 100 people, so we can&rsquo;t let<br/>everyone in. Let me just check if your name is on the list&hellip;</p></blockquote></p>

<p><blockquote><p>Ok man, no problem&hellip;name&rsquo;s Greg Sestero!</p></blockquote></p>

<p><blockquote><p>&hellip;mmm&hellip;</p></blockquote></p>

<p><blockquote><p>Is there any problem?</p></blockquote></p>

<p><blockquote><p>&hellip;mmm&hellip;</p></blockquote></p>

<p><blockquote><p>&hellip;hello?</p></blockquote></p>

<p><blockquote><p>Damn Greg, give me some time! I have to go through 100 names to find yours!</p></blockquote></p>

<p>Quickly, Tommy realizes that this is going to take him too long, so he comes over
and starts telling me that we need to find another way to figure out whether people
have signed up or not: with hundreds of people in front of him, he can&rsquo;t go over the list
for each and everyone of them!</p>

<p><blockquote><p>Alex, man&hellip;I can&rsquo;t go over the list for each and everyone out there. We need to<br/>find a way to check if these people signed up way faster than this, as it&rsquo;s taking<br/>me too long! At this pace, we&rsquo;ll start the hackathon in 2 hours!</p></p><p><p>Even if it&rsquo;s not 100% accurate, we need to find another method of processing<br/>each and everyone&hellip;at the end of the day we don&rsquo;t care if we let in 98 or 106 people,<br/>but we need to do it real quick!</p></blockquote></p>

<p><blockquote><p>Tommy, we can&rsquo;t turn people down if they subscribed, I&rsquo;d be pretty pissed if that<br/>happened to me! We should let everyone who subscribed in, and maybe have some<br/>false positives so that a couple people who didn&rsquo;t subscribe make it in&hellip;but again,<br/>there&rsquo;s no way we can deny entry to people who actually subscribed!</p></blockquote></p>

<p><blockquote><p>So, false positives are ok but false negatives aren&rsquo;t&hellip;I think I&rsquo;ve got this!</p></blockquote></p>

<p><blockquote><p>Tell me Tommy, what are you thinking about?</p></blockquote></p>

<p><blockquote><p>You remember in our signup form we asked people for their birthday? Can you quickly<br/>print me a list of all years, months and days people were born in? It should look like:</p></p><p><p>YEAR<br/>1978<br/>1979<br/>1984<br/>1985<br/>1986<br/>&hellip;</p></p><p><p>MONTH<br/>01<br/>03<br/>04<br/>05<br/>07<br/>&hellip;</p></p><p><p>DAY<br/>01<br/>02<br/>09<br/>10<br/>11<br/>12<br/>15<br/>17<br/>&hellip;</p></p><p><p>I&rsquo;m gonna ask people what year they were born.<br/>If it&rsquo;s in the list, I&rsquo;m going to ask them what month.<br/>If it&rsquo;s in the list, I&rsquo;m going to ask them what day.</p></p><p><p>If they&rsquo;re all in the list, there&rsquo;s a good chance these guys signed up! We will<br/>definitely have false positives, but I don&rsquo;t have to go through a list of 100 names,<br/>just, maybe, 20 years, 10 months and 20 days&hellip;it&rsquo;s still half of the original list<br/>of names!</p></blockquote></p>

<p><blockquote><p>You&rsquo;re a genius! Not an exact science but we should be able to get this going!</p></blockquote></p>

<p>Bloom filters work a little bit differently, but the idea behind them is quite
similar to the one I just presented &mdash; let&rsquo;s get more technical!</p>

<h2>The technical bits&hellip;</h2>

<p>&hellip;because we&rsquo;re literally talking about <strong>bits</strong>! Instead of birth dates, a Bloom
filter simply uses a bit array such as:</p>

<table>
<thead>
<tr>
<th> 0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0</th>
</tr>
</thead>
<tbody>
</tbody>
</table>


<p>Now, like in a regular set, let&rsquo;s start adding elements to the filter: we first need to pick a hashing
function (like <a href="https://en.wikipedia.org/wiki/MurmurHash">murmur</a>) and then <strong>flip a bit to 1 when the hashing function returns
the index of such bit</strong>.</p>

<p>For example, the string <code>some_test</code> would return <code>2</code> in an evenly distributed murmur
(you can test it <a href="http://murmurhash.shorelabs.com/">here</a>), so we flip the element
at index <code>2</code>:</p>

<table>
<thead>
<tr>
<th>0 </th>
<th> 0 </th>
<th> 1 </th>
<th>0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0</th>
</tr>
</thead>
<tbody>
</tbody>
</table>


<p>Let&rsquo;s add another element to the filter &mdash; <code>another_test</code> hashes to <code>4</code>:</p>

<table>
<thead>
<tr>
<th>0 </th>
<th> 0 </th>
<th> 1 </th>
<th>0 </th>
<th> 1 </th>
<th> 0 </th>
<th>0 </th>
<th> 0 </th>
<th> 0 </th>
<th>0</th>
</tr>
</thead>
<tbody>
</tbody>
</table>


<p>And now let&rsquo;s test the bloom filter! Is <code>I_DONT_EXISTS</code> in the filter?</p>

<p>Taken from <a href="http://murmurhash.shorelabs.com/">murmurhash.shorelabs.com</a>:</p>

<p><img class="center" src="/images/murmur.png"></p>

<p>This element hashes to <code>5</code>, and the value of the element at index <code>5</code> in our
filter is <code>0</code>. This means that the element is not in the original set.</p>

<p>Let&rsquo;s do another test? Let&rsquo;s see if <code>not_exist_no_no</code> is in the filter:</p>

<p><img class="center" src="/images/murmur2.png"></p>

<p>This element hashes to <code>2</code>, and the value of the element at index <code>2</code> in our
filter is <code>1</code>. This would suggest that the element was in our original set &mdash;
here&rsquo;s where Bloom filters trick you.</p>

<p>A Bloom filter guarantees certainty when it tells you an element is not in a set,
but can only give you a probability of whether the element was in it. This means
that you can reliably use these filters to exclude values from a set, but are not
going to be 100% sure they are in the set if the filter returns a positive result
&mdash; in other words: <strong>Bloom filters never give you false negatives, but can give you false
positives</strong>.</p>

<p>If you&rsquo;re still having some trouble to understand how they work (I&rsquo;m not the best at this, I&rsquo;ll admit it)
I would strongly encourage you to have a look at this <a href="http://llimllib.github.io/bloomfilter-tutorial/">interactive explanation of Bloom filters</a>.</p>

<h2>Even MOAR technical bits!</h2>

<p>Bloom filters are actually different &mdash; I tend to
oversimplify for the sake of understanding. The two most important differences from
the approach I explained above are that:</p>

<ul>
<li>the length of the bit array needs to be &ldquo;quite&rdquo; proportional (more on this later) to the number
of elements in the original set, as a set with 100 elements represented in a filter
with only 2 bits would be useless (there&rsquo;s an incredibly strong chance both bits
would be positive, increasing the chances of false positives)</li>
<li>multiple hashing functions are used to increase entropy within the distribution
of 1-value bits, as <a href="https://www.quora.com/Why-do-bloom-filters-have-multiple-hash-functions">a single hashing function could lead to a higher number of collisions</a></li>
</ul>


<p>There are generally four variables you want to keep an eye on when using a Bloom
filter:</p>

<ul>
<li>the number of items in the filter (= number of items in the original set, <strong>n</strong>)</li>
<li>the probability of false positives (<strong>p</strong>)</li>
<li>number of bits in the filter (<strong>m</strong>)</li>
<li>number of hash functions used to convert element into bits (<strong>k</strong>)</li>
</ul>


<p>You start by knowing (or by having a good estimate of) your <strong>n</strong> and defining an
acceptable <strong>p</strong>; at that point you derive <strong>m</strong> and <strong>k</strong> by:</p>

<ul>
<li><code>m = ceil((n * log(p)) / log(1.0 / (pow(2.0, log(2.0)))))</code></li>
<li><code>k = round(log(2.0) * m / n)</code></li>
</ul>


<p>We can test this out by using <a href="https://hur.st/bloomfilter">this interesting calculator</a> made available by
<a href="https://hur.st/">Thomas Hurst</a>:</p>

<table>
<thead>
<tr>
<th>items (n) </th>
<th> probability (p) </th>
<th> space (m) </th>
<th> hash fn (k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>100 </td>
<td> 0.5 </td>
<td> 145b </td>
<td> 1</td>
</tr>
<tr>
<td>100 </td>
<td> 0.1 </td>
<td> 480b </td>
<td> 3</td>
</tr>
<tr>
<td>1k </td>
<td> 0.5 </td>
<td> 175B </td>
<td> 1</td>
</tr>
<tr>
<td>1k </td>
<td> 0.1 </td>
<td> 600B </td>
<td> 3</td>
</tr>
<tr>
<td>1M </td>
<td> 0.1 </td>
<td> 600KB </td>
<td> 3</td>
</tr>
<tr>
<td>1M </td>
<td> 0.01 </td>
<td> 1.2MB </td>
<td> 7</td>
</tr>
<tr>
<td>1M </td>
<td> 0.001 </td>
<td> 1.7MB </td>
<td> 10</td>
</tr>
</tbody>
</table>


<p>Now it&rsquo;s the &ldquo;show me some code!&rdquo; time, so let&rsquo;s grab the <a href="https://www.npmjs.com/package/bloomfilter">bloomfilter</a>
package on NPM and <a href="https://github.com/odino/bloom-test">let&rsquo;s run some benchmarks</a>.
We will first create a list of a 10k elements and check 1k random elements against it:</p>

<p>``` js
originalSet = generateSet(10000)
tests = generateSet(1000)</p>

<p>console.time(&lsquo;plain&rsquo;)
for (test of tests) {
  if (originalSet.includes(test)) {</p>

<pre><code>// HERE BE DRAGONS!
</code></pre>

<p>  }
}
console.timeEnd(&lsquo;plain&rsquo;)
```</p>

<p>and let&rsquo;s try to compare it with a high-precision (0.01% probability of false positives)
filter:</p>

<p>``` js
let BloomFilter = require(&lsquo;bloomfilter&rsquo;).BloomFilter</p>

<p>originalSet = generateSet(10000)
tests = generateSet(1000)</p>

<p>// m=10k,p=0.01
var bloom = new BloomFilter(
  95851, // number of bits to allocate.
  7        // number of hash functions.
);</p>

<p>for (elem of originalSet) {
  bloom.add(elem)
}</p>

<p>console.time(&lsquo;bloom-high-precision&rsquo;)
for (test of tests) {
  if (bloom.test(test)) {</p>

<pre><code>// HERE BE DRAGONS!
</code></pre>

<p>  }
}
console.timeEnd(&lsquo;bloom-high-precision&rsquo;)
```</p>

<p>For the lolz, let&rsquo;s also have a very imprecise bloom filter thrown into the mix:</p>

<p><code>js
// m=10k,p=0.5
var bloom = new BloomFilter(
  14427, // number of bits to allocate.
  1        // number of hash functions.
);
</code></p>

<p>&hellip;and let&rsquo;s run them together:</p>

<p><code>bash
plain: 29.210ms
bloom-high-precision: 0.694ms
bloom-yolo: 0.357ms
</code></p>

<p>Quite of a difference! Such performance gains are quite understandable since the
filter does not need to loop through the set (<code>O(n)</code>), it just needs to hash the
element it has received as many times as the number of hash functions in the filter (<code>O(k)</code>)
and then access the list at the index produced by the hash functions (<code>O(1)</code>), ending
up at a complexity of <code>O(k)</code> &mdash; which is pretty darn fast!</p>

<p>Another important thing to consider is, beside the time-advantage,
is that <strong>the space advantage of Bloom filters is quite significant</strong>: since they don&rsquo;t need
to store the original elements of the set, but just a fixed-length bit sequence, they tend
to consume less space than hash tables, tries or plain lists.</p>

<p>An interesting thing to consider is how these filters relate to hash tables:</p>

<p><blockquote><p>hash tables gain a space and time advantage if they begin ignoring collisions and store only whether each bucket contains an entry; in this case, they have effectively become Bloom filters with k = 1</p><footer><strong>Wikipedia <a href="https://en.wikipedia.org/wiki/Bloom_filter#Space_and_time_advantages">https://en.wikipedia.org/wiki/Bloom_filter#Space_and_time_advantages</a> Space and Time advantages of Bloom filters</strong></footer></blockquote></p>

<h2>A note on hashing functions</h2>

<p>It&rsquo;s really important to pick the right hashing functions when implementing a Bloom
filter, as they need to:</p>

<ul>
<li><strong>be extremely fast</strong>: strong cryptographic hashing functions are not suitable since
they are generally computationally expensive (= slow), while you will want your
<code>O(k)</code> to be as fast as possible</li>
<li><strong>distribute outputs uniformly</strong>: it&rsquo;s important for the filter to &ldquo;turn bits
on&rdquo; as uniformly as possible, as a &ldquo;biased&rdquo; hash function would end up increasing
the probability of false positive</li>
</ul>


<p>For example, a unanimous choice seems to be <a href="https://en.wikipedia.org/wiki/MurmurHash">MurMurHash3</a>,
which guarantees a good degree of uniformity in terms of distribution and was
designed with speed, <a href="https://en.wikipedia.org/wiki/List_of_hash_functions#Non-cryptographic_hash_functions">not security</a>, in mind.</p>

<h2>Closing remarks</h2>

<p>Have you ever visited a website and seen a security notice from your browser,
telling you the URL you&rsquo;re about to access could be malicious? How do you think
your browser can efficiently check whether the URL is among a huge list of
malicious ones?</p>

<p>Well, up until a few years ago <a href="http://blog.alexyakunin.com/2010/03/nice-bloom-filter-application.html">Chrome used Bloom filters</a>
and that&rsquo;s just one interesting, beneficial application of this amazingly clever
data structure (if you&rsquo;re curious, it since <a href="https://bugs.chromium.org/p/chromium/issues/detail?id=71832">switched to prefix sets</a> &mdash;
&ldquo;Searches take about 3x as long, but it should scale well&rdquo;).</p>

<p>To give you some more context, Bloom filters appeared in July 1970 &mdash; that is 7 months into the unix timestamp.</p>

<p>Here I sit speechless.</p>

<h2>Further readings</h2>

<ul>
<li><a href="https://dl.acm.org/citation.cfm?doid=362686.362692">the original paper</a></li>
<li><a href="https://blog.medium.com/what-are-bloom-filters-1ec2a50c68ff">What are Bloom filters (and how Medium uses them)?</a></li>
<li><a href="https://cloud.google.com/blog/big-data/2017/05/after-lambda-exactly-once-processing-in-cloud-dataflow-part-2-ensuring-low-latency">After Lambda: Exactly-once processing in Cloud Dataflow</a></li>
<li><a href="http://blog.kiip.me/engineering/sketching-scaling-bloom-filters/">Sketching &amp; Scaling: Bloom Filters</a></li>
<li><a href="http://www.michaelnielsen.org/ddi/why-bloom-filters-work-the-way-they-do/">Why Bloom filters work the way they do</a></li>
<li><a href="https://bdupras.github.io/filter-tutorial/">Probablistic Filters Visualized</a></li>
<li><a href="https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf">Cuckoo Filter: Practically Better Than Bloom</a></li>
<li><a href="https://eugene-eeo.github.io/blog/bloom-filter-explained.html">Bloom Filters Explained</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[My Favorite Algorithm (and Data Structure): HyperLogLog]]></title>
    <link href="https://odino.org/my-favorite-data-structure-hyperloglog/"/>
    <updated>2018-01-13T21:09:00+00:00</updated>
    <id>https://odino.org/my-favorite-data-structure-hyperloglog</id>
    <content type="html"><![CDATA[<p>Every now and then I bump into a concept that&rsquo;s so simple and powerful that I want
to stab my brain for missing out on such an incredible and beautiful idea.</p>

<p>I discovered <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a> (HLL) a
couple years ago and fell in love with it right after reading how
<a href="http://antirez.com/news/75">redis decided to add a HLL data structure</a>:
the idea behind HLL is devastatingly simple but extremely powerful, and it&rsquo;s
what makes it such a widespread algorithm, used by giants of the internet such
as Google and Reddit.</p>

<!-- more -->


<h2>So, what&rsquo;s your phone number?</h2>

<p>My friend Tommy and I planned to go to a conference and, while heading to
its location, decide to wager on who will talk to the most strangers.
So once we reach the place we start conversing around and keep a counter of
how many people we talk to.</p>

<p><img class="center" src="/images/networking-event.png"></p>

<p>At the end of the event Tommy comes to me with his figure (17) and I tell him
that I had a word with 46 people: I clearly am the winner, but Tommy&rsquo;s frustrated
as he thinks I&rsquo;ve counted the same people multiple times, as he only saw me with
15/20 people in total. So, the wager&rsquo;s off and we decide that,
for our next event, we&rsquo;ll be taking down names instead, so that we&rsquo;re sure we&rsquo;re
going to be counting unique people, and not just the total number of conversations.</p>

<p>At the end of the following conference we enthusiastically meet each other with
a very long list of names and, guess what, Tommy had a couple more encounters
than I did! We both laugh it off and while discussing our approach to counting
uniques, Tommy comes up with a great idea:</p>

<p><blockquote><p>Alex, you know what? We can&rsquo;t go around with pen and paper and track down a list of names, it&rsquo;s really impractical!<br/>Today I spoke to 65 different people and counting their names on this paper was a real pain in the back&hellip;I lost count 3 times and had to start from scratch!</p></blockquote></p>

<p><blockquote><p>Yeah, I know, but do we even have an alternative?</p></blockquote></p>

<p><blockquote><p>What if, for our next conference, instead of asking for names, we ask people the last 5 digits of their phone number?</p></p><p><p>Now, follow me: instead of winning by counting their names, the winner will be the one who spoke to someone with the longest sequence of leading zeroes in those digits.</p></blockquote></p>

<p><blockquote><p>Wait Tommy, you&rsquo;re going too fast! Slow down a second and give me an example&hellip;</p></blockquote></p>

<p><blockquote><p>Sure, just ask people for those last 5 digits, ok? Let&rsquo;s suppose you get 54701.<br/>No leading zero, so the longest sequence of zeroes for you is 0.</p></p><p><p>The next person you talk to tells you it&rsquo;s 02561 &mdash; that&rsquo;s a leading zero! So your longest sequence comes to 1.</p></blockquote></p>

<p><blockquote><p>You&rsquo;re starting to make sense to me&hellip;</p></blockquote></p>

<p><blockquote><p>Yeah, so if we speak to a couple people, chances are that are longest zero-sequence will be 0. But if we talk to ~10 people, we have more chances of it being 1.</p></p><p><p>Now, imagine you tell me your longest zero-sequence is 5 &mdash; you must have spoken to thousands of people to find someone with 00000 in their phone number!</p></blockquote></p>

<p><blockquote><p>Dude, you&rsquo;re a damn genius!</p></blockquote></p>

<p>And that, my friends, is how HyperLogLog fundamentally works: it allows us to
estimate uniques within a large dataset by recording the longest sequence of
zeroes within that set. This ends up creating an incredible advantage over keeping
track of each and every element in the set, making it an incredibly efficient way
to count unique values with relatively high accuracy:</p>

<p><blockquote><p>The HyperLogLog algorithm can estimate cardinalities well beyond 10<sup>9</sup> with a relative accuracy (standard error) of 2% while only using 1.5kb of memory.</p><footer><strong>Fangjin Yang <a href="http://druid.io/blog/2012/05/04/fast-cheap-and-98-right-cardinality-estimation-for-big-data.html">http://druid.io/blog/2012/05/04/fast-cheap-and-98-right-cardinality-estimation-for-big-data.html</a> Fast</strong> <cite>Cheap</cite></footer></blockquote></p>

<p>Since this is the usual me oversimplifying things
that I find hard to understand, let&rsquo;s have a look at some more details of HLL.</p>

<h2>More HLL details</h2>

<p>HLL is part of a family of algorithms that aim to address
<a href="https://en.wikipedia.org/wiki/Count-distinct_problem">cardinality estimation</a>,
otherwise known as <em>count-distinct problem</em>,
which are extremely useful for lots of today&rsquo;s web applications &mdash; for example
when you want to count how many unique views an article on your site has generated.</p>

<p>When HLL runs, it takes your input data and hashes it, turning into a bit
sequence:</p>

<p>```
IP address of the viewer: 54.134.45.789</p>

<p>HLL hash: 010010101010101010111010&hellip;
```</p>

<p>Now, an important part of HLL is to make sure that your hashing function
distributes bits as evenly as possible, as you don&rsquo;t want to use a weak function
such as:</p>

<p>`&ldquo; js
function hash(ip) {
  let h = &rdquo;</p>

<p>  ip.replace(/\D/g,&lsquo;&rsquo;).split(&lsquo;&rsquo;).forEach(number => {</p>

<pre><code>h += number &lt; 5 ? 0 : 1
</code></pre>

<p>  })</p>

<p>  return h
}
```</p>

<p>A HLL using this hashing function would return biased results if, for example,
the <a href="https://stackoverflow.com/a/277537/934439">distribution of your visitors is tied to a specific geographic region</a>.</p>

<p>The <a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">original paper</a>
has a few more details on what a good hashing function means for HLL:</p>

<p><blockquote><p>All known efficient cardinality estimators rely on randomization, which is ensured by the use of hash functions.</p></p><p><p>The elements to be counted belonging to a certain data domain D, we assume given a hash function, h : D â {0, 1}â; that is, we assimilate hashed values to infinite binary strings of {0, 1}â, or equivalently to real numbers of the unit interval.</p></p><p><p>[&hellip;]</p></p><p><p>We postulate that the hash function has been designed in such a way that the hashed values closely resemble a uniform model of randomness, namely, bits of hashed values are assumed to be independent and to have each probability [0.5] of occurring.</p><footer><strong>Philippe Flajolet <a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf</a> HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm</strong></footer></blockquote></p>

<p>Now, after we&rsquo;ve picked a suitable hash function we need to address another pitfall:
<a href="https://en.wikipedia.org/wiki/Variance">variance</a>.</p>

<p>Going back to our example, imagine that the first person you talk to at the conference
tells you their number ends with <code>00004</code> &mdash; jackpot! You might have won a wager
against Tommy, but if you use this method in real life chances are that specific
data in your set will negatively influence the estimation.</p>

<p>Fear no more, as this is <strong>a problem HLL was born to solve</strong>: not many are aware that <a href="https://en.wikipedia.org/wiki/Philippe_Flajolet">Philippe
Flajolet</a>, one of the brains behind HLL, was quite involved in cardinality-estimation
problems for a long time, long enough to have come up with the <a href="https://en.wikipedia.org/wiki/Flajolet%E2%80%93Martin_algorithm#Improving_accuracy">Flajolet-Martin
algorithm in 1984</a> and
<a href="http://algo.inria.fr/flajolet/Publications/DuFl03-LNCS.pdf">(super-)LogLog in 2003</a>,
which already addressed some of the problems with outlying hashed values by dividing
measurements into buckets, and (somewhat) averaging values across buckets.</p>

<p>If you got lost here, let me go back to our original example: instead of just
taking the last 5 digits of a phone number, we take 6 of them and store the longest
sequence of leading zeroes together with the first digit (the bucket). This means
that our data will look like:</p>

<p>```
Input:
708942 &mdash;> in the 7th bucket, the longest sequence of zeroes is 1
518942 &mdash;> in the 5th bucket, the longest sequence of zeroes is 0
500973 &mdash;> in the 5th bucket, the longest sequence of zeroes is now 2
900000 &mdash;> in the 9th bucket, the longest sequence of zeroes is 5
900672 &mdash;> in the 9th bucket, the longest sequence of zeroes stays 5</p>

<p>Buckets:
0: 0
1: 0
2: 0
3: 0
4: 0
5: 2
6: 0
7: 1
8: 0
9: 5</p>

<p>Output:
avg(buckets) = 0.8
```</p>

<p>As you see, if we weren&rsquo;t employing buckets we would instead use 5 as the longest
sequence of zeroes, which would negatively impact our estimation: even though I
simplified the math behind buckets (it&rsquo;s not just a simple average), you can
totally see how this approach makes sense.</p>

<p>It&rsquo;s interesting to see how Flajolet addresses variance throughout his
works:</p>

<p><blockquote><p>While we&rsquo;ve got an estimate that&rsquo;s already pretty good, it&rsquo;s possible to get a lot better. Durand and Flajolet make the observation that outlying values do a lot to decrease the accuracy of the estimate; by throwing out the largest values before averaging, accuracy can be improved.</p></p><p><p>Specifically, by throwing out the 30% of buckets with the largest values, and averaging only 70% of buckets with the smaller values, accuracy can be improved from 1.30/sqrt(m) to only 1.05/sqrt(m)! That means that our earlier example, with 640 bytes of state and an average error of 4% now has an average error of about 3.2%, with no additional increase in space required.</p></p><p><p>Finally, the major contribution of Flajolet et al in the HyperLogLog paper is to use a different type of averaging, taking the harmonic mean instead of the geometric mean we just applied. By doing this, they&rsquo;re able to edge down the error to  1.04/sqrt(m), again with no increase in state required.</p><footer><strong>Nick Johnson <a href="http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation">http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation</a> Improving accuracy: SuperLogLog and HyperLogLog</strong></footer></blockquote></p>

<h2>HLL in the wild</h2>

<p>So, where can we find HLLs? Two great web-scale examples are:</p>

<ul>
<li><a href="https://cloud.google.com/blog/big-data/2017/07/counting-uniques-faster-in-bigquery-with-hyperloglog">BigQuery</a>,
to efficiently count uniques in a table (<code>APPROX_COUNT_DISTINCT()</code>)</li>
<li><a href="https://redditblog.com/2017/05/24/view-counting-at-reddit/">Reddit</a>, where it&rsquo;s used to calculate how many unique views a post has gathered</li>
</ul>


<p>In particular, see how HLL impacts queries on BigQuery:</p>

<p><code>`` bash
SELECT COUNT(DISTINCT actor.login) exact_cnt
FROM</code>githubarchive.year.2016`</p>

<blockquote><p>6,610,026 (4.1s elapsed, 3.39 GB processed, 320,825,029 rows scanned)</p></blockquote>

<p>SELECT APPROX_COUNT_DISTINCT(actor.login) approx_cnt
FROM <code>githubarchive.year.2016</code></p>

<blockquote><p>6,643,627 (2.6s elapsed, 3.39 GB processed, 320,825,029 rows scanned)
```</p></blockquote>

<p>The second result is an approximation (with an error rate of ~0.5%), but takes
a fraction of the time.</p>

<p>Long story short: <strong>HyperLogLog is amazing!</strong> You now know what it is and when it can be
used, so go out and do incredible stuff with it!</p>

<h2>Just before you leave&hellip;</h2>

<p>One thing I&rsquo;d like to clarify is that even though I&rsquo;ve referred to HLL as a data structure before, it
should be noted that it is an algorithm first, while some databases (eg. Redis, Riak, BigQuery)
have implemented their own data structures based on HLL (so while saying HLL is a data structure is technically incorrect, it&rsquo;s also not
entirely wrong).</p>

<h2>Further readings</h2>

<ul>
<li><a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog on Wikipedia</a></li>
<li>the <a href="http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf">original paper</a></li>
<li><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/40671.pdf">HyperLogLog++, Google&rsquo;s improved implementation of HLL</a></li>
<li><a href="http://antirez.com/news/75">Redis new data structure: the HyperLogLog</a></li>
<li><a href="http://blog.notdot.net/2012/09/Dam-Cool-Algorithms-Cardinality-Estimation">Damn Cool Algorithms: Cardinality Estimation</a></li>
<li><a href="https://github.com/basho/riak_kv/blob/develop/docs/hll/hll.pdf">HLL data types in Riak</a></li>
<li><a href="http://tech.adroll.com/blog/data/2013/07/10/hll-minhash.html">HyperLogLog and MinHash</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Probabilistic Data Structures: An Introduction]]></title>
    <link href="https://odino.org/probabilistic-data-structures-an-introduction/"/>
    <updated>2018-01-12T09:44:00+00:00</updated>
    <id>https://odino.org/probabilistic-data-structures-an-introduction</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/dice.png"></p>

<p>In the past few years I&rsquo;ve got more and more accustomed to computer science
concepts that were foreign to me earlier in my career: one of the most interesting
aspects that I&rsquo;ve focused on is <a href="https://dzone.com/articles/introduction-probabilistic-0">probabilistic data structures</a>,
which I want to cover with a few posts in the upcoming months.</p>

<p>My excitement around these structures come from the fact that they
enable us to accomplish tasks that were impractical before, and can really influence
the way we design software &mdash; for example, Reddit counts unique views with a
probabilistic data structure as it lets them scale more efficiently.</p>

<!-- more -->


<h2>What&rsquo;s all the fuss about?</h2>

<p>Let&rsquo;s get practical very quickly &mdash; imagine you have a set of records and want to calculate if an element is part
of that set:</p>

<p>``` js
let set = new Array()</p>

<p>for (let x = 0; x &lt; 10; x++) {
  set.push(<code>test${x}</code>)
}</p>

<p>console.log(set.includes(&lsquo;a&rsquo;)) // false
```</p>

<p>Here we are loading the entire set in memory and then loop (<code>.includes(...)</code>)
over it to figure out if our element is part of it.</p>

<p>Let&rsquo;s say we want to figure out the resources used by this script:</p>

<p>``` js
console.time(&lsquo;search&rsquo;)
let set = new Array()</p>

<p>for (let x = 0; x &lt; 10; x++) {
  set.push(<code>test${x}</code>)
}</p>

<p>console.log(set.includes(&lsquo;a&rsquo;))
console.timeEnd(&lsquo;search&rsquo;)
console.log(<code>~${process.memoryUsage().heapUsed / 1000000}mb</code>)</p>

<p>// output:
// false
// search: 2.184ms
// ~4.448288mb
```</p>

<p>As you can see, the time spent in running the script is minimal, and memory is
also &ldquo;low&rdquo;. What happens when we beef up our original list?</p>

<p><code>`` js
...
for (let x = 0; x &lt; 10000000; x++) {
  set.push(</code>test${x}`)
}
&hellip;</p>

<p>// output:
// false
// search: 2383.794ms
// ~532.511032mb
```</p>

<p>See, the figures change quite drastically &mdash; it&rsquo;s not even the execution time
that should scare you (most of the time is spent in filling the array, not in the
<code>.includes(...)</code>), but rather the amount of memory the process is consuming: as usual,
the more data we use, the more memory we consume (no shit, Sherlock!).</p>

<p>This is exactly the problem that probabilistic data structures try to solve, as you:</p>

<ul>
<li>might not have enough available resources</li>
<li>might not need a precise answer to your problem</li>
</ul>


<p>If you can trade certainty off for the sake of staying lightweight, a <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filter</a>
would, for example, be the right data structure for this particular use case:</p>

<p>``` js
const BloomFilter = require(&lsquo;bloomfilter&rsquo;).BloomFilter
console.time(&lsquo;search&rsquo;)
var bloom = new BloomFilter(
  287551752, // number of bits to allocate.
  20        // number of hash functions.
);</p>

<p>for (let x = 0; x &lt; 10000000; x++) {
  bloom.add(<code>test${x}</code>)
}</p>

<p>console.log(bloom.test(&lsquo;a&rsquo;))
console.timeEnd(&lsquo;search&rsquo;)
console.log(<code>~${process.memoryUsage().heapUsed / 1000000}mb</code>)</p>

<p>// output
// false
// search: 11644.863ms
// ~10.738632mb
```</p>

<p>In this case the bloom filter has given us the same output (with a <a href="https://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives">degree
of certainty</a>) while using 10MB of RAM rather than 500MB.
Oh, boy!</p>

<p>This is exactly what probabilistic data structures help you with: you need an
answer with a degree of certainty and don&rsquo;t care if they&rsquo;re off by a tiny
bit &mdash; because to get an exact answer you would require an impractical amount of
resources.</p>

<p>Who cares if that video has been seen by 1M unique users or 1.000.371 ones? If
you find yourself in this situation, chances are that a probabilistic structure
would fit extremely well in your architecture.</p>

<h2>Next steps</h2>

<p>I have only really started to scratch the surface of what is possible thanks to
probabilistic data structures but, if you are fascinated as much as I am, you
will find some of my next articles interesting enough, as I am planning to cover
the ones that I understand better in the upcoming weeks &mdash; namely <a href="https://en.wikipedia.org/wiki/HyperLogLog">HyperLogLog</a>
(by far my favorite data structure) and
<a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</a>.</p>

<p>The papers behind these data structures are pretty <em>math-heavy</em> and I do not understand half of that jazz :)
so we&rsquo;re going to take a look at them with more of a simplistic, practical view
than a theoretical one.</p>

<h2>Just before you leave&hellip;</h2>

<p>One thing I want to clarify: the specific numbers you&rsquo;ve seen in this post will vary from platform to platform, so don&rsquo;t
look at the absolute numbers but rather at the magnitude of the difference. Also,
here I just focused on one application of Bloom filters, which demonstrates their
advantage in terms of space complexity, but time complexity should be accounted
for as well &mdash; that&rsquo;s material for another post!</p>

<p>Cheers!</p>
]]></content>
  </entry>
  
</feed>
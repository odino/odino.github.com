<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Category: Microservices | Alessandro Nadalin]]></title>
  <link href="https://odino.org/categories/microservices/atom.xml" rel="self"/>
  <link href="https://odino.org/"/>
  <updated>2022-11-18T09:33:53+00:00</updated>
  <id>https://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Istio Is the Real Deal]]></title>
    <link href="https://odino.org/istio-is-the-real-deal/"/>
    <updated>2018-03-23T14:06:00+00:00</updated>
    <id>https://odino.org/istio-is-the-real-deal</id>
    <content type="html"><![CDATA[<p>I don&rsquo;t generally spend time blogging (or bragging) about technologies I don&rsquo;t
use in production, but today I wanted to make sure <a href="https://gist.github.com/odino/7ce4dc3bb77d6282b2f2aaf1050c29e3">Istio</a> gets a special mention on
this blog.</p>

<!-- more -->


<p>The original problem was: how do I package and run apps with ease? And Docker came.</p>

<p>Then it was a matter of how do we organize all of these containers. And Kubernetes came.</p>

<p>Last but not least, we now have to sort out how to make services communicate
to each other, how to route them conditionally and secure this layer of communication.
That&rsquo;s where Istio, a <a href="https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/">service mesh</a> for microservices developed <a href="https://developer.ibm.com/dwblog/2017/istio/">Google, IBM and Lyft</a>, kicks in.</p>

<p>Some of the cool things you can do with this service mesh?</p>

<ul>
<li><a href="https://istio.io/docs/tasks/policy-enforcement/rate-limiting.html">rate limiting</a></li>
<li><a href="https://istio.io/docs/concepts/traffic-management/handling-failures.html">circuit breakers</a></li>
<li><a href="https://istio.io/docs/concepts/traffic-management/rules-configuration.html#timeouts-and-retries">auto-retry API calls</a></li>
<li><a href="https://istio.io/blog/2017/0.1-canary.html">canary releases</a></li>
<li><a href="https://istio.io/docs/reference/config/istio.mixer.v1.config.client.html#JWT">JWT authn/authz</a></li>
</ul>


<p>&hellip;and much, much more.</p>

<p>I&rsquo;ll leave you with this talk by Kelsey Hightower &mdash; I&rsquo;m sure that, with time,
Istio will blow your mind:</p>

<iframe width="860" height="615" src="https://www.youtube.com/embed/s4qasWn_mFc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Better Performance: The Case for Timeouts]]></title>
    <link href="https://odino.org/better-performance-the-case-for-timeouts/"/>
    <updated>2017-01-19T18:57:00+00:00</updated>
    <id>https://odino.org/better-performance-the-case-for-timeouts</id>
    <content type="html"><![CDATA[<p>Most of the larger-scale services that we design
nowadays depend, more or less, on external APIs:
you&rsquo;ve heard it multiple times, as soon as your codebase
starts to look like a monolith it&rsquo;s time to start
splitting it into <a href="https://en.wikipedia.org/wiki/Microservices">smaller services</a>
that can evolve independently and aren&rsquo;t strongly
coupled with the monolith.</p>

<p>Even if you don&rsquo;t really employ microservices, chances
are that you already depend on external services, such
as <a href="https://en.wikipedia.org/wiki/Elasticsearch">elasticsearch</a>,
<a href="https://redis.io/">redis</a> or a payment gateway, and need to integrate
with them via some kind of APIs.</p>

<p>What happens when those services are slow or unavailable? Well,
you can&rsquo;t process search queries, or payments, but your
app would still be working &ldquo;fine&rdquo; &mdash; right?</p>

<p><strong>That is not always the case</strong>, and I want to run a few
benchmarks to show you how a little tweak, <a href="https://en.wikipedia.org/wiki/Timeout_(computing">timeouts</a>,
prove beneficial when dealing with external services.</p>

<!-- more -->


<h2>Our case</h2>

<p>We&rsquo;ve started a new <em>Hello World!</em> startup that,
surprisingly, makes money by deploying a useless
service that prints a string retrieved from another
service: as you understand,
this is an oversimplification of a real-world
scenario, but it will serve our purpose well enough.</p>

<p><img class="center" src="/images/a-b-service.png"></p>

<p>Our clients will be connecting to our main frontend,
<code>server1.js</code> which will then make an HTTP request towards
another service, <code>server2.js</code> which will reply back:
once we have an answer from <code>server2.js</code>  we can then
return the response body to our client.</p>

<p>``` js server1.js
let http = require(&lsquo;http&rsquo;)</p>

<p>http.createServer((req, res) => {
  require(&lsquo;unirest&rsquo;).get(&lsquo;<a href="http://localhost:3001">http://localhost:3001</a>&rsquo;).end(function&reg; {</p>

<pre><code>res.writeHead(r.error ? 500 : 200)
res.write(r.error ? r.error.message : r.body)
res.end()
</code></pre>

<p>  })
}).listen(3000)
```</p>

<p>``` js server2.js
let http = require(&lsquo;http&rsquo;)</p>

<p>http.createServer((req, res) => {
  setTimeout(_ => {</p>

<pre><code>res.writeHead(200)
res.write('Hello!')
res.end()  
</code></pre>

<p>  }, 100)
}).listen(3001)
```</p>

<p>A few things to note:</p>

<ul>
<li>the servers run on port <code>3000</code> (main app) and <code>3001</code> (&ldquo;backend&rdquo; server): so once the client will make a request to <code>localhost:3000</code> a new HTTP request will be sent to <code>localhost:3001</code></li>
<li>the backend service will wait 100ms (this is to simulate real-world use cases) before returning a response</li>
<li>I&rsquo;m using the <a href="https://github.com/Mashape/unirest-nodejs">unirest</a> HTTP client: I like it a lot and, even though we could have simply used the built-in <code>http</code> module, I&rsquo;m confident this will gives us a better feeling in terms of real-world applications</li>
<li>unirest is nice enough to tell us if there was an error on our request, so we can just check <code>response.error</code> and handle the drama from there</li>
<li>I&rsquo;m going to be using <a href="https://www.docker.com/">docker</a> to run these tests, and the code is <a href="https://github.com/odino/the-case-for-timeouts">available on github</a></li>
</ul>


<h2>Let&rsquo;s run our first tests</h2>

<p>Let&rsquo;s run our servers and start bombing <code>server1.js</code> with
requests: we&rsquo;ll be using <a href="https://www.joedog.org/siege-home/">siege</a> (I&rsquo;m too hipster for <a href="https://httpd.apache.org/docs/2.4/programs/ab.html">AB</a>),
which provides some useful info upon executing the load test:</p>

<p>```
siege -c 5 www.google.com
<strong> SIEGE 3.0.5
</strong> Preparing 5 concurrent users for battle.
The server is now under siege&hellip;^C
Lifting the server siege&hellip;      done.</p>

<p>Transactions:                 26 hits
Availability:             100.00 %
Elapsed time:               6.78 secs
Data transferred:           0.20 MB
Response time:              0.52 secs
Transaction rate:           3.83 trans/sec
Throughput:             0.03 MB/sec
Concurrency:                2.01
Successful transactions:          27
Failed transactions:               0
Longest transaction:            1.28
Shortest transaction:           0.36
```</p>

<p>The <code>-c</code> option, in siege, defines how many concurrent requests
we should send to the server, and you can even specify how many
repetitions (<code>-r</code>) you&rsquo;d like to run: for example, <code>-c 10 -r 5</code>
would mean we&rsquo;d be sending to the server 50 total requests, in
batches of 10 concurrent requests. For the purpose of our benchmark
I decided, though, to keep the tests running for 3 minutes and
analyze the results afterwards, without setting a max number of
repetitions.</p>

<p>Additionally, in my next examples I will be trimming down the results
to the most important items provided by siege:</p>

<ul>
<li>availability: how many of our requests was the server able to handle</li>
<li>transaction rate: how many requests per second we were able to male</li>
<li>successful / failed transactions: how many requests ended up with successful / failure status codes (ie. 2xx vs 5xx)</li>
</ul>


<p>Let&rsquo;s start by sending 500 concurrent requests to observe how the services
behave.</p>

<p>```
docker run &mdash;net host -v $(pwd):/src -d mhart/alpine-node:7.1 node /src/server1.js
docker run &mdash;net host -v $(pwd):/src -d mhart/alpine-node:7.1 node /src/server2.js</p>

<p>siege -c 500 127.0.0.1:3000
```</p>

<p>After around 3 minutes, it&rsquo;s time to stop siege (<code>ctrl+c</code>) and see how the results
look like:</p>

<p><code>
Availability:             100.00 %
Transaction rate:        1156.89 trans/sec
Successful transactions:      205382
Failed transactions:               0
</code></p>

<p>Not bad, as we&rsquo;ve been able to serve 1156 transactions per second;
even better than that, it doesn&rsquo;t seem like we&rsquo;ve got any
error, which means our success rate is 100%.
What if we up our game and go for 1k concurrent transactions?</p>

<p>```
siege -c 1000 127.0.0.1:3000
&hellip;</p>

<p>Availability:             100.00 %
Transaction rate:        1283.61 trans/sec
Successful transactions:      232141
Failed transactions:               0
```</p>

<p>Well, well done: we slightly increased throughput, as now our app
is able to handle 1283 requests per second: since the apps
do very less (print a string and that&rsquo;s it) it is likely that
the more concurrent requests we&rsquo;ll send the higher the throughput.</p>

<p>These numbers might be useless now (we&rsquo;re not comparing them
to anything) but will prove essential in the next paragraphs.</p>

<h2>Introducing failure</h2>

<p>This is not how real-world webservices behave: you have
to <strong>accept failures</strong> and build resilient applications
that are capable of overcoming them.</p>

<p>For example, suppose our backend service is going through
a hard phase and starts lagging every now and then:</p>

<p>``` js server2.js
let http = require(&lsquo;http&rsquo;)</p>

<p>let i = 0</p>

<p>http.createServer((req, res) => {
  let delay = 100
  if (i % 10 === 0) {</p>

<pre><code>delay = 10000
</code></pre>

<p>  }</p>

<p>  i++</p>

<p>  setTimeout(_ => {</p>

<pre><code>res.writeHead(200)
res.write('Hello!')
res.end()
</code></pre>

<p>  }, delay)
}).listen(3001)
```</p>

<p>In this example, 1 out of 10 requests will be served after a
timeout of 10s has passed, whereas the other ones will be
processed with the &ldquo;standard&rdquo; delay of 100ms: this kind of
simulates a scenario where we have multiple servers behind
a load balancer, and one of them starts throwing random
errors or becomes slower due to excessive load.</p>

<p>Let&rsquo;s go back to our benchmark and see how our <code>server1.js</code>
performs now that its dependency will start to slow down:</p>

<p>```
siege -c 1000 127.0.0.1:3000</p>

<p>Availability:             100.00 %
Transaction rate:         853.93 trans/sec
Successful transactions:      154374
Failed transactions:               0
```</p>

<p><strong>What a bummer</strong>: our transaction rate has plummeted,
down by more than 30%, just because some of the responses
are lagging &mdash; this means that <code>server1.js</code> needs
to hold on for longer in order to receive responses
from <code>server2.js</code>, thus using more resources and being
able to serve less requests than it theoretically can.</p>

<h2>An error now is better than a response tomorrow</h2>

<p>The case for timeouts starts by recognizing one simple
fact: <strong>users won&rsquo;t wait for slow responses</strong>.</p>

<p>After 1 or 2 seconds, their attention will fade away and the
chances that they might still be hooked to your content
will vanish as soon as you cross the 4/5s threshold &mdash;
this mean that <strong>it&rsquo;s generally better to give them
an immediate feedback,
even if negative</strong> (&ldquo;<em>An error occurred, please try again</em>&rdquo;),
rather than letting them get frustrated over how slow your
service is.</p>

<p>In the spirit of &ldquo;<a href="https://en.wikipedia.org/wiki/Fail-fast">fail fast</a>&rdquo;, we decide to add a timeout
in order to make sure that our responses meet a certain SLA:
in this case, we decide that our SLA is 3s, which is the
time our users will possibly wait to use our service<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>.</p>

<p>``` js server1.js
&hellip;</p>

<p>require(&lsquo;unirest&rsquo;).get(&lsquo;<a href="http://localhost:3001">http://localhost:3001</a>&rsquo;).timeout(3000).end(function&reg; {</p>

<p>&hellip;
```</p>

<p>Let&rsquo;s see how the numbers look like with timeouts enabled:</p>

<p><code>
Availability:              90.14 %
Transaction rate:        1125.26 trans/sec
Successful transactions:      209861
Failed transactions:           22964
</code></p>

<p>Oh boy, we&rsquo;re back into the game: the transaction rate is
again higher than 1k per second and we can almost serve
as many requests as we&rsquo;d do under ideal conditions (when
there&rsquo;s no lag in the backend service).</p>

<p>Of course, one of the drawbacks is that we have now
increased the number of failures (10% of total requests),
which means that some
users will be presented an error page &mdash; still better,
though, than serving them after 10s, as most of them
would have abandoned our service anyway.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p>Let&rsquo;s dig into it.</p>

<h2>The RAM factor</h2>

<p>In order to figure out how much memory our <code>server1.js</code>
is consuming, we need to measure, at intervals, the amount
of memory the server is using; in production, we would
use tools such as <a href="https://newrelic.com/">NewRelic</a> or <a href="https://keymetrics.io/">KeyMetrics</a>
but, for our simple scripts, we&rsquo;ll resort to the poor man&rsquo;s
version of such tools: we&rsquo;ll be printing the amount of
memory from <code>server1.js</code> and we&rsquo;ll use another script
to record the output and print some simple stats.</p>

<p>Let&rsquo;s make sure <code>server1.js</code> prints the amount of memory
used every 100ms:</p>

<p>``` js server1.js
&hellip;</p>

<p>setInterval(_ => {
  console.log(process.memoryUsage().heapUsed / 1000000)
}, 100)</p>

<p>&hellip;
```</p>

<p>If we start the server we should see something like:</p>

<p><code>
3.990176
4.066752
4.076024
4.077784
4.079544
4.081304
4.083064
4.084824
</code></p>

<p>which is the amount of memory, in MB, that the server is
using: in order to crunch the numbers I wrote a simple script
that reads the input from the <code>stdin</code> and computes the stats:</p>

<p>``` js
let inputs = []</p>

<p>process.stdin.on(&lsquo;data&rsquo;, (data) => {
  inputs.push(parseInt(data.toString(), 10))</p>

<p>  let min = Math.round(Math.min(&hellip;inputs))
  let max = Math.round(Math.max(&hellip;inputs))
  let avg = Math.round(inputs.reduce((a, b) => a + b) / inputs.length)
  let cur = Math.round(inputs[inputs.length &ndash; 1])</p>

<p>  process.stdout.write(<code>Meas: ${inputs.length} Min: ${min} Max: ${max} Avg: ${avg} Cur: ${cur}\r</code>)
});
```</p>

<p>The module is <a href="https://github.com/odino/node-number-aggregator-stats">public</a>
and <a href="https://www.npmjs.com/package/number-aggregator-stats">available on NPM</a>,
so we can just install it globally and redirect the output of the server to it:</p>

<p>```
docker run &mdash;net host -v $(pwd):/src -ti mhart/alpine-node:7.1 sh
npm install -g number-aggregator-stats
node /src/server1.js | number-aggregator-stats</p>

<p>Meas: 18 Min: 3 Max: 4 Avg: 4 Cur: 4
```</p>

<p>Let&rsquo;s now run our benchmark again &mdash; 3 minutes, 1k
concurrent requests, no timeouts:</p>

<p>```
node /src/server1.js | number-aggregator-stats</p>

<p>Meas: 1745 Min: 3 Max: 349 Avg: 194 Cur: 232
```</p>

<p>And now let&rsquo;s enable the 3s timeout:</p>

<p>```
node /src/server1.js | number-aggregator-stats</p>

<p>Meas: 1429 Min: 3 Max: 411 Avg: 205 Cur: 172
```</p>

<p>Whoa, at a first look it seems like <strong>timeouts aren&rsquo;t helping
after all: our memory usage hits a high with timeouts enabled
and is, on average, 5% higher as well</strong>. Is there any reasonable
explanation to that?</p>

<p>There is, of course, as we just need to go back to siege and
look at the rps:</p>

<p><code>
853.60 trans/sec --&gt; without timeouts
1134.48 trans/sec --&gt; with timeouts
</code></p>

<p><img class="right" src="/images/vegeta.jpg"></p>

<p>Here we&rsquo;re comparing <strong>apples to oranges</strong>: it&rsquo;s useless
to look at memory usage of 2 servers that are serving
a different number of rps, as we should instead
make sure they are both offering the same throughput,
and only measure the memory at that point, else the
server that&rsquo;s serving more requests will always start
with some disadvantage!</p>

<p>To do so, we need some kind of tool that makes it easy
to generate rps-based load, and siege is not very
suitable for that: it&rsquo;s time to call our friend <a href="https://github.com/tsenart/vegeta">vegeta</a>,
a modern load testing tool written in <a href="https://golang.org/">golang</a>.</p>

<h2>Enter vegeta</h2>

<p>Vegeta is very simple to use, just start &ldquo;attacking&rdquo;
a server and let it report the results:</p>

<p><code>
echo "GET http://google.com" | vegeta attack --duration 1h -rate 1000 | tee results.bin | vegeta report
</code></p>

<p>2 very interesting options here:</p>

<ul>
<li><code>--duration</code>, so that vegeta will stop after a certain time</li>
<li><code>--rate</code>, as in rps</li>
</ul>


<p>Looks like vegeta is the right tool for us &mdash; we can then issue
a command tailored to our server and see the results:</p>

<p><code>
echo "GET http://localhost:3000" | vegeta attack --duration 3m --insecure -rate 1000 | tee results.bin | vegeta report
</code></p>

<p>This is what vegeta outputs without timeouts:</p>

<p><code>
Requests      [total, rate]            180000, 1000.01
Duration      [total, attack, wait]    3m10.062132905s, 2m59.998999675s, 10.06313323s
Latencies     [mean, 50, 95, 99, max]  1.172619756s, 170.947889ms, 10.062145485s, 10.134037994s, 10.766903205s
Bytes In      [total, mean]            1080000, 6.00
Bytes Out     [total, mean]            0, 0.00
Success       [ratio]                  100.00%
Status Codes  [code:count]             200:180000  
Error Set:
</code></p>

<p>and this is what we get when <code>server1.js</code> has
the 3s timeout enabled:</p>

<p><code>
Requests      [total, rate]            180000, 1000.01
Duration      [total, attack, wait]    3m3.028009507s, 2m59.998999479s, 3.029010028s
Latencies     [mean, 50, 95, 99, max]  455.780741ms, 162.876833ms, 3.047947339s, 3.070030628s, 3.669993753s
Bytes In      [total, mean]            1142472, 6.35
Bytes Out     [total, mean]            0, 0.00
Success       [ratio]                  90.00%
Status Codes  [code:count]             500:18000  200:162000  
Error Set:
500 Internal Server Error
</code></p>

<p>as you see, the total number of requests and
elapsed time is the same between the 2
benchmarks, meaning that we&rsquo;ve put the servers
under the same level of stress: now that we&rsquo;ve
got them to perform the same tasks, under the
same load, we can look at memory usage to see
if timeouts helped us keeping a lower memory
footprint.</p>

<p>Without timeouts:</p>

<p>```
node /src/server1.js | number-aggregator-stats</p>

<p>Meas: 1818 Min: 3 Max: 372 Avg: 212 Cur: 274
```</p>

<p>and with timeouts:</p>

<p>```
node /src/server1.js | number-aggregator-stats</p>

<p>Meas: 1886 Min: 3 Max: 299 Avg: 149 Cur: 292
```</p>

<p>This looks more like it: timeouts have
helped us keeping the <strong>memory usage, on average,
30% lower</strong><sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup>.</p>

<p>All of this thanks to a simple <code>.timeout(3000)</code>:
what a win!</p>

<h2>Avoiding the domino effect</h2>

<p>Quoting myself:</p>

<p><blockquote><p>What happens when those services are slow or unavailable? Well,<br/>you can&rsquo;t process search queries, or payments, but your<br/>app would still be working &ldquo;fine&rdquo; &mdash; right?</p></blockquote></p>

<p>Fun fact: <strong>a missing timeout can
cripple your entire infrastructure!</strong></p>

<p><img class="left" src="/images/domino.jpg"></p>

<p>In our basic example we saw how a service that
starts to fail at a 10% rate can significantly
increase memory
usage of the services depending on it, and that&rsquo;s
not an unrealistic scenario &mdash; it&rsquo;s basically
just a single, wonky server in a pool of ten.</p>

<p>Imagine you have a webpage that relies on a
backend service, behind a load balancer, that
starts to perform slower than usual: the service
still works (it&rsquo;s just way slower than it should),
your healthcheck is probably
still getting a <code>200 Ok</code> from the service (even
though it comes after several seconds rather than
milliseconds), so the service won&rsquo;t be removed from
the load balancer.</p>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<h2>A note on timeouts</h2>

<p>If you thought waiting is dangerous, let&rsquo;s add to the
fire:</p>

<ul>
<li>we&rsquo;re not talking HTTP only &mdash; everytime we rely on an external system we should <a href="https://github.com/mysqljs/mysql#connection-options">use timeouts</a></li>
<li>a server could have an open port and drop every packet you send &mdash; this will result in a TCP connection timeout. Try this in your terminal: <code>time curl example.com:81</code>. Good luck!</li>
<li>a server could reply instantly, but be very slow at sending each packet (as in, seconds between packets). You would then need to protect yourself against a <strong>read timeout</strong></li>
</ul>


<p>&hellip;and many more edge cases to list. I know, distributed systems are nasty.</p>

<p>Luckily, high-level APIs (like the one <a href="https://github.com/Mashape/unirest-nodejs#requesttimeoutnumber">exposed by unirest</a>)
are generally helpful since they take care of all of the hiccups that
might happen on the way.</p>

<h2>Closing remarks: I&rsquo;ve broken every single benchmarking rule</h2>

<p>If you have any &ldquo;aggressive&rdquo; feedback about my <em>rusty</em> benchmarking skills&hellip;
&hellip;well, I would agree with you, as I purposely took some shortcuts
for the sake of simplifying my job and the ability, for you, to
easily reproduce these benchmarks.</p>

<p>Things you should do if you&rsquo;re serious about benchmarks:</p>

<ul>
<li>do not run the code you&rsquo;re benchmarking and the tool you use to benchmark on the same machine. Here I ran everything on my XPS which is powerful enough to let me run these tests, though running siege / vegeta on the same machine the servers run definitely has an impact on the results (I say <code>ulimit</code> and you figure out the rest). My advice is to try to get some hardware on AWS and benchmark from there &mdash; more isolation, less doubts</li>
<li>do not measure memory by logging it out with a <code>console.log</code>, instead use a tool such as NewRelic which, I think, is less invasive</li>
<li>measure more data: benchmarking for 3 minutes is ok for the sake of this post, but if we want to look at real-world data, to give a better estimate of how helpful timeouts are, you should leave the benchmarks running for way longer</li>
<li>keep Gmail closed while you run <code>siege ...</code>, the tenants living in <code>/proc/cpuinfo</code> will be grateful</li>
</ul>


<p>And&hellip;I&rsquo;m done for the day: I hope you enjoyed this post and, if otherwise, feel
free to rant in the comment box below!</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>For system-to-system integrations it could even be 10 to 20 seconds, depending on how slow the backend you have to connect to is (sigh: don&rsquo;t ask me) <a href='#fnref:1' rev='footnote'>↩</a></li><li id='fn:2'>But don&rsquo;t be naive and think that the 30% of memory saved will be the same in your production infrastructure. It really depends on your setup &mdash; it could be lower (most likely) or even higher. Benchmark for yourself and see what you&rsquo;re missing out on! <a href='#fnref:2' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[On Monoliths, Service-oriented Architectures and Microservices]]></title>
    <link href="https://odino.org/on-monoliths-service-oriented-architectures-and-microservices/"/>
    <updated>2015-02-20T19:48:00+00:00</updated>
    <id>https://odino.org/on-monoliths-service-oriented-architectures-and-microservices</id>
    <content type="html"><![CDATA[<p>In the last few years web architectures have
been evolving pretty fast, and the result is
that now we have a few approaches to pick
between when building your next software
architecture.</p>

<!-- more -->


<h2>Monoliths</h2>

<p><img class="right" src="/images/monolith.png"></p>

<p>Monolithic architectures are the ones
running on a single application layer
that tends to bundle together all the functionalities
needed by the architecture.</p>

<p>At the architectural level, this is the
simplest form of architecture simply
because it doesn&rsquo;t involve
as many actors as other architectural
styles.</p>

<p>If we, for example, want to build a web
architecture with a monolithic approach,
we would start developing the frontend of it and
make it access data directly rather than
giving it an abstraction layer such as an API.</p>

<p>Most setups, nowadays, run through a monolithic
approach as, for small/mid sized architectures,
it runs pretty well and keeps complexity quite
low: the <strong>problems tend to come when the
architecture needs to scale up feature-wise</strong>;
modules are extensively dependent to each other,
the code becomes hard to refactor as
it involves touching the whole monolith (think,
for example, of doing an extensive refactoring on
the <code>HttpRequest</code> class, which would impact
every single request to every single functionality
of the architecture).</p>

<p>I personally recommend monoliths for projects with
a <strong>very small and &ldquo;easy&rdquo; scope</strong>, where you don&rsquo;t
need too much abstraction and won&rsquo;t likely have
to maintain or evolve the codebase year after
year.</p>

<h2>Semi-monoliths</h2>

<p><img class="left" src="/images/semi-monoliths.png"></p>

<p>These are a bit tricky: I consider semi-monliths
decoupled architecture that actually really on
<strong>smaller, but still large, monoliths</strong>.</p>

<p>This actually means that you think of, for example
for performance reasons, decoupling your frontend
from your backend but end up building 2 (or maybe
even 3) large applications that turn into monoliths
on their own.</p>

<p>I find semi-monoliths <strong>quite harmful</strong> as they are,
in my opinion, <strong>a wrong step in the right direction</strong>:
you believe decoupling works well but do
just a bit of it, ending up with the same problems
you&rsquo;d have with monoliths, just on a bigger stage.</p>

<p>In complex architectures it&rsquo;s usually much easier
to keep each piece simple but small, whereas semi-monoliths
end up solving just the surface of a problem (ie. performance
bottlenecks) but then leave you with the complexity
of each mini-monolith.</p>

<p>I personally see very few scenarios in which
semi-monoliths are a very good choice: my rule of thumb
is that <strong>if the scope of the architecture is small
you can use a (small) app, else be wild and use
a service for each functionality</strong>, without limiting
yourself to the frontend vs backend thingy.</p>

<h2>SOA</h2>

<p><img class="right" src="/images/lego-soa.png"></p>

<p>SOAs (Service-Oriented Architectures) are a way
to &ldquo;properly&rdquo; evolve from semi-monoliths to a more
diversified architecture.</p>

<p>SOAs usually incorporate functions into small/mid-sized
applications (more on this later), <strong>a lot of them</strong>: you try to
keep the complexity of each app / functionality very low
and make them communicate over a set APIs (being them
HTTP APIs, asynchronous messaging and so on);
the services do multiple things all of which are
limited to the scope of a single functionality,
for example customer management.</p>

<p>I like to describe SOAs as<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>:</p>

<blockquote><p>A software design based on discrete software components,
&ldquo;services&rdquo;, that collectively provide the functionalities
of the larger software architecture</p></blockquote>

<p>A clear disadvantage of SOAs is that it might
be overkill: I still remember my first advice
about SOAs at the CakeFest in San Francisco 2
years ago, &ldquo;<a href="http://www.slideshare.net/odino/tips-and-tricks-for-your-service-oriented-architecture-cakefest-2013-in-san-francisco/50">avoid SOA</a>&rdquo;.</p>

<p>If you don&rsquo;t need to separate functionalities of
your architecture, simply don&rsquo;t do it: tipically
SOAs are reserved to complex products and systems
that cannot be summed up in a briefing; I clearly
remember, back at the time when I was working in
an agency, that none of our projects was really
suitable for full blown SOAs because the scope was
so limited that there would be no reason to introduce
complexity (at the architectural level).</p>

<p>On the other end, I&rsquo;d recommend to buy into several services
as soon as you realize that there is too much
complexity in the architecture: you will definitely
understand when this happens because you have clearly
defined boundaries between software components, you
start to realize that one piece of the architecture
shouldn&rsquo;t bring down the rest of it if a deployment
goes wrong and so on. In other terms, <strong>you&rsquo;ll feel it</strong>.</p>

<p>SOAs usually give you a good flexibility but, as said, come
at a cost: even though each piece has its own life,
evolves independently, doesn&rsquo;t impact the others
very much and it&rsquo;s simples, <strong>the architecture itself
becomes more complex</strong>.</p>

<p>One thing to clarify about SOAs is that, nowadays,
the term has lost its appeal due to the fact that
it <a href="http://martinfowler.com/bliki/ServiceOrientedAmbiguity.html">might mean too many things</a>:
SOA is a very general term and it&rsquo;s hard to pinpoint
what it actually represents, though, in recent years,
it seems that we all agreed that a very good way to
do SOA is through microservices.</p>

<h2>Microservices</h2>

<p><img class="right" src="/images/ants-microservices.png"></p>

<p>And here we are with <a href="http://martinfowler.com/articles/microservices.html">microservices</a>.
What do microservice-based architectures actually are?</p>

<p>I could say a lot of things but, to simplify, I will
pick my own definition:</p>

<blockquote><p>microservice-based architectures are the ones
that mimic SOAs with very small, <a href="http://en.wikipedia.org/wiki/Unix_philosophy">unix-inspired</a> services,
which do one thing and do it well</p></blockquote>

<p>What does that actually mean? Shrink those services,
make them as small and indipendent as possible and
create a hell lot of them; the only difference I see
between traditional SOAs and microservices is that
the latter clearly states that the size of a
service should be minimal, else it needs to be split
in multiple services. In other words, <strong>microservices
are an implementation of SOA</strong>.</p>

<p>As we&rsquo;ve seen with traditional SOAs, microservices bring
a lot of complexity at the architectural level as there
are even more, tiny actors involved, but the practical
advantage is that they are all isolated, indipendent
and only communicate through simple interfaces (any
kind of API).</p>

<p>This piece sums my thoughts up quite well:</p>

<p><blockquote><p>[microservices are] one form of SOA,<br/>perhaps service orientation done right</p><footer><strong>Martin Fowler on microservices <a href="http://martinfowler.com/articles/microservices.html">http://martinfowler.com/articles/microservices.html</a></strong></footer></blockquote></p>

<p>I am a fan of simplicity and good abstraction, which
means that you should have clear, neat boundaries and
APIs between your services but also should not forget
of avoiding bloating or shrinking them too much, else you end
up overengineering in both cases.</p>

<p>Using microservices also requires quite of a shift in terms
of mindset as there are a lot of things that change
in your development lifecycle: things need to be simple,
well documented, smooth and easy to run; imagine the next
guy coming to your team that, to fix a bug, has to learn
how to run 6 (micro)services together&hellip;hell! That is why
you need to figure a solution out to allow fast development
cycles and simplicity to run, deploy and evolve<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup> those
small services.</p>

<p>So yes, microservices add complexity at the architectural
level (where you&rsquo;d likely be happy to have it) with the
advantage of outrageously <strong>simplifying each software
component</strong>, which makes it simple, for anyone, to get used
to the architecture day after day, feature after feature,
service after service.</p>

<h2>So what?</h2>

<p>I hope this clarifies some terminology and decisions
you might wanna take when building your next (big or
small) project; there will always be a lot of external
factors, like timeline or resources, to keep in
consideration but I believe it&rsquo;s very important to know
about your options.</p>

<p>Since we are talking about web architectures, I&rsquo;d like
to leave you with a pearl on <a href="http://www.aol.com/">AOL</a>
from <a href="http://highscalability.com/blog/2014/2/17/how-the-aolcom-architecture-evolved-to-99999-availability-8.html">highscalabilty</a>:</p>

<p><blockquote><p>The architecture for AOL.com is in it’s 5th generation.</p></p><p><p>It has essentially been rebuilt from scratch 5 times over two decades.</p></p><p><p>The current architecture was designed<br/>6 years ago. Pieces have been upgraded and new componentshave been added along the way, but the overall design remains largely intact.</p></p><p><p>The code, tools, development and deployment processes are highly tuned over 6 years of continual improvement, making the AOL.com architecture battle tested and very stable.</p><footer><strong>Dave Hagler</strong> <cite>Systems Architect at AOL</cite></footer></blockquote></p>

<p>Have fun with your next architecture folks!</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>I&rsquo;m not sure if this definition is purely mine or if I read it somewhere &mdash; pardon my lack of memory! <a href='#fnref:1' rev='footnote'>↩</a></li><li id='fn:2'>This last point, evolving, is taking care by the architectural style itself. Small, indipendent services are easy to evolve by definition, as they are not complex <a href='#fnref:2' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
</feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Category: K8s | Alessandro Nadalin]]></title>
  <link href="https://odino.org/categories/k8s/atom.xml" rel="self"/>
  <link href="https://odino.org/"/>
  <updated>2021-05-20T10:16:37+00:00</updated>
  <id>https://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Local K8s Development in 2020]]></title>
    <link href="https://odino.org/local-k8s-development-in-2020/"/>
    <updated>2019-12-31T15:45:00+00:00</updated>
    <id>https://odino.org/local-k8s-development-in-2020</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/kubernetes-logo.png"></p>

<p>This decade&rsquo;s about to wrap up, so I decided to spend some time
describing my development workflow as the year nears its end.</p>

<p>What I find interesting in my setup is that it entertains
working on a local k8s cluster &mdash; mainly to keep in touch
with the systems that run in production.</p>

<p>Running k8s locally isn&rsquo;t what
you&rsquo;d want to do to begin with, but rather a natural path
once you start wanting to replicate the environment that runs
your live applications. Again, you don&rsquo;t need a local k8s
cluster just &lsquo;cause, so make sure you have a good reason
before going through the rest of this article.</p>

<!-- more -->


<h2>Cluster setup</h2>

<p>Once a monstrous task, setting up a local k8s cluster is
now as simple as installing a package on your system:
Docker for Win/Mac allow you to run this very easily, and
Canonical has made it possible on Linux through <a href="https://microk8s.io/">microk8s</a>
(that&rsquo;s my boy!).</p>

<p>One of the funny things about running on microk8s
(or snaps, in general) is how it will automagically
upgrade under your nose &mdash; sometimes with breaking changes.
There was a recent change that <a href="https://github.com/ubuntu/microk8s/issues/382">swapped docker for containerd
as microk8s' default container runtime</a>, and it broke some
local workflows (more on that later, as it&rsquo;s easy to fix).
In general, you can always force a snap to use a particular
revision, so if anything&rsquo;s funky just downgrade and let others
figure it out :)</p>

<p>I&rsquo;d be keen to try <a href="https://k3s.io/">k3s</a> out, as it seem to provide an even more
lightweight way to run the local cluster. Built mainly for
IoT and edge computing, k3s is interesting as running
microk8s is sometime resource-intesive &mdash; once I&rsquo;m done
working on code, I usually prefer to <code>sudo snap disable microk8s</code>
in order to preserve RAM, CPU and battery life (<a href="https://github.com/odino/dev/commit/20749dd50c590ec376b7eed2db558615f1ef6fda">proof here</a>).</p>

<p>In the past, I&rsquo;ve also tried to work on a remote k8s cluster
in the GKE from my local machine, but that proved to be too
much of a hassle &mdash; the beauty of <code>kubectl</code> is that you don&rsquo;t
really care where the cluster is running, but your IDE and
other tools work best when everything is present and running
locally.</p>

<h2>Development tool</h2>

<p>This has been fairly stable until late this year, when I
decided to switch things around.</p>

<p>I&rsquo;ve historically used helm and a bunch of shell magic
to run apps locally: you would clone a repo and expect
an <code>helm/</code> folder to be available, with the chart being
able to install a whole bunch of k8s resources on your
cluster. Then, a bash script run simply apply the chart
with a bunch of pre-configured values: you would run <code>dev up</code>
and what the script would do would simply be something
along the lines of:</p>

<p><code>
docker build -t $CURRENT_FOLDER .
helm install --name $CURRENT_FOLDER ./helm/ --set mountPath=/home/alex/path/to/$CURRENT_FOLDER --set image=$CURRENT_FOLDER
kubectl logs -f deploy/$CURRENT_FOLDER --tail=100
</code></p>

<p>Nothing too crazy&hellip;but with a few downsides:</p>

<ul>
<li>I started off with Helm 2, and v3 brought in a few changes I didn&rsquo;t want to go through</li>
<li>helm is perfect if I want to package a generic app made up of multiple resources (service, ingress, etc) and release it to the outside world. Locally, I probably don&rsquo;t need all of that verbosity (<code>chart.yaml</code> and so on)</li>
<li>most of the templating I did on development was <code>{{ .Release.name }}</code>. What&rsquo;s the point then?</li>
</ul>


<p>Towards the end of this year I went back to the drawing board
and started to think what if there was anything else I could
use that was simple enough and gave me enough flexibility.
I knew I could use simple k8s manifests but it wasn&rsquo;t clear
to me how I could integrate it into my workflow in a way
that made it simpler than using a chart &mdash; and that&rsquo;s when I
gave <a href="https://skaffold.dev/">skaffold</a> another chance.</p>

<p>Skaffold is an interesting tool, promoted by Google, that
supposedly handles local k8s development &mdash; and I say &ldquo;supposedly&rdquo;
because I&rsquo;ve tried it in the past and have been extremely
underwhelmed by its workflow.</p>

<p>Let me explain: whenever a chance is detected in your codebase, skaffold
wants to redeploy your manifests but, rather than simply
working on an application-reload logic, is instead happy
to:</p>

<ul>
<li>re-build your local image</li>
<li>push it to a registry</li>
<li>update the k8s deployment so a brand new pod comes up</li>
</ul>


<p>If you&rsquo;ve made it so far you probably realized that
the whole operation doesn&rsquo;t either come cheap nor fast
&mdash; you could be waiting several seconds for your changes
to take effect&hellip;</p>

<p>That was, until skaffold introduced <a href="https://skaffold.dev/docs/pipeline-stages/filesync/">file sync</a>
to avoid the need to rebuild, redeploy and restart pods.
This feature is currently in beta, but it&rsquo;s already working
well enough that I&rsquo;ve decided to give it a shot, with very
positive results.</p>

<p>Now, rather than having an entire chart to mantain locally,
my development setup has a simple <code>skaffold.yaml</code> that
looks like:</p>

<p>```yml
apiVersion: skaffold/v1
kind: Config
build:
  artifacts:
  &ndash; image: my_app</p>

<pre><code>context: .
sync:
  manual:
    - src: "**"
      dest: "/src"
</code></pre>

<p>deploy:
  kubectl:</p>

<pre><code>manifests:
  - skaffold/*.yaml
</code></pre>

<p>```</p>

<p>and I reduced the manifests to a simple k8s manifest
containing multiple resources, separated by <code>---</code>:</p>

<p>```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my_app
  labels:</p>

<pre><code>app: my_app
</code></pre>

<p>spec:
  replicas: 1
  selector:</p>

<pre><code>matchLabels:
  app: my_app
</code></pre>

<p>  template:</p>

<pre><code>metadata:
  name: my_app
  labels:
    app: my_app
spec:
  restartPolicy: Always
  containers:
  - image: my_app
    name: my_app
    imagePullPolicy: IfNotPresent
    tty: true
    stdin: true
    env:
    - name: ENV
      value: dev
  terminationGracePeriodSeconds: 1
</code></pre>

<hr />

<p>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my_app
  annotations:</p>

<pre><code>kubernetes.io/ingress.class: nginx
</code></pre>

<p>spec:
  tls:</p>

<pre><code>- hosts:
    - my-app.dev
</code></pre>

<p>  rules:
  &ndash; host: my-app.dev</p>

<pre><code>http:
  paths:
  - path: /
    backend:
      serviceName: my_app
      servicePort: http
</code></pre>

<ul>
<li>host: my-app.dev
<a href="http:">http:</a>
  paths:

<ul>
<li>path: /
backend:
  serviceName: my_app
  servicePort: http</li>
</ul>
</li>
</ul>


<hr />

<p>apiVersion: v1
kind: Service
metadata:
  name: my_app
spec:
  ports:
  &ndash; name: http</p>

<pre><code>port: 80
protocol: TCP
targetPort: 8080
</code></pre>

<p>  selector:</p>

<pre><code>app: my_app
</code></pre>

<p>```</p>

<p>That&rsquo;s about it. Now my <code>dev up</code> is mapped to a simple
<code>skaffold dev</code>, and skaffold takes care of re-building
the image when needed, syncing changes locally and so on.
One of the advantages of using this tool is that it automatically
detects changes to the manifests and the Dockerfile, so
it re-builds the image without you having to trigger the
process manually (which wasn&rsquo;t possible with Helm alone).</p>

<p>Another interesting benefit of using skaffold is the support
for base registries as well as build stages. The former
allows you to run a registry at any given URL, and tell
skaffold to prepend that URL to any image that&rsquo;s being pushed
to the k8s cluster.</p>

<p>As I mentioned, I use microk8s, which doesn&rsquo;t play very well
with <a href="https://microk8s.io/docs/registry-images">locally-built images</a>,
so I simply run the built-in registry on port <code>32000</code>. Others
in my team simply run Docker for Mac which doesn&rsquo;t need a registry
as any image built locally is automatically available to k8s.</p>

<p>This would mean that I would have to update the <code>image</code> field
of my deployments, manually, to <code>localhost:32000/my_app</code>, a
tedious and annoying operation (and I&rsquo;d also have to make sure
those changes aren&rsquo;t pushed to git). Skaffold frees you from the
drama with a simple <code>skaffold config set default-repo localhost:32000</code>,
a trick that will tell skaffold to parse all the manifests it deploys
and replace the <code>image</code> fields, prepending the URL of your own registry.
The feature is documented extensively <a href="https://skaffold.dev/docs/environment/image-registries/">here</a>,
and it&rsquo;s a life saver!</p>

<p>The support for build stages is another great trick up in
skaffold&rsquo;s sleeve, as it allows to use the power of Docker&rsquo;s
multi-stage builds in your development environment.</p>

<p>If you have a Dockerfile that looks like:</p>

<p>```
FROM golang:1.13 as dev</p>

<p>RUN go get -v github.com/codegangsta/gin
WORKDIR /src
COPY go.mod /src
COPY go.sum /src
RUN go mod download
COPY . /src</p>

<p>RUN go build -o my_go_binary main.go
CMD gin run main.go</p>

<p>FROM gcr.io/distroless/base as prod
COPY &mdash;from=dev /src /
CMD [&ldquo;/my_go_binary&rdquo;]
```</p>

<p>You can tell skaffold that, locally, it should simply stop at the
<code>dev</code> stage:</p>

<p>```yml
apiVersion: skaffold/v1
kind: Config
build:
  artifacts:
  &ndash; image: my_go_app</p>

<pre><code>docker:
  target: dev
context: .
sync:
  manual:
    - src: "**"
      dest: "/src"
</code></pre>

<p>deploy:
  kubectl:</p>

<pre><code>manifests:
  - skaffold/*.yaml
</code></pre>

<p>```</p>

<p>As you see, the <code>target</code> field does the trick.</p>

<p>Believe me, skaffold has made my life so much easier and it&rsquo;s a tool
I would gladly recommend. Before introducing file syncing I didn&rsquo;t
want to get my hads dirty with it, as I did not find the development
workflow sustainable (re-build and re-deploy at every file change),
but right now it works much better than anything I could have come
up with on my own.</p>

<h2>Hands on the code</h2>

<p>Last but not least, we went over running a cluster as well as our
application &mdash; but how do we actually debug our code or run tests?</p>

<p>Ideally, we&rsquo;d like a script that would be able to:</p>

<ul>
<li>build and run your app (<code>up</code>)</li>
<li>execute commands inside the container (<code>exec</code>)</li>
<li>jump inside the container (<code>in</code>)</li>
<li>execute tests (<code>test</code>)</li>
</ul>


<p>Wouldn&rsquo;t it be nice to simply open a shell and run your tests with <code>dev test</code>?</p>

<p>Turns out, creating a simple wrapper over our wokflow is very
straightforward, and here&rsquo;s a sample of the code one could write:</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<h1>dev $command</h1>

<p>arg=$1
command=&ldquo;cmd_$1&rdquo;</p>

<h1>Boot the app through skaffold.</h1>

<p>cmd_up(){</p>

<pre><code>skaffold dev
</code></pre>

<p>}</p>

<h1>Deletes the app</h1>

<p>cmd_delete(){</p>

<pre><code>skaffold delete
</code></pre>

<p>}</p>

<h1>This will drop you inside the container.</h1>

<h1>We use bash, if available, else &ldquo;sh&rdquo;</h1>

<p>cmd_in(){</p>

<pre><code>app=$(get_app_name)
kubectl exec -ti deploy/$app -- sh -c "command -v bash &amp;&amp; bash || sh"
</code></pre>

<p>}</p>

<h1>Read the app name based on the image we</h1>

<h1>build inside the skaffold.yaml</h1>

<p>get_app_name(){
  echo $(yq read skaffold.yaml build.artifacts.0.image)
}</p>

<h1>Main function: if a command has been passed,</h1>

<h1>check if it&rsquo;s available, and execute it. If</h1>

<h1>the command is not available, we print the</h1>

<h1>default help.</h1>

<p>main(){</p>

<pre><code>declare -f $command &gt; /dev/null

if [ $? -eq 0 ]
then
    $command $@
else
    printf "'$arg' is not a recognized command.\n\n"
    exit 1
fi
</code></pre>

<p>}</p>

<p>shift
main $@
```</p>

<p>All this script does is to read the command passed to it
and run it as a bash function. As you see, <code>up</code> is mapped
to <code>skaffold dev</code>, and <code>in</code> is mapped to <code>kubectl exec ... -- bash</code>
(so that you can jump into the container and run whatever
command you&rsquo;d like).</p>

<p>The actual <code>dev</code> I run locally is on github, under <a href="https://github.com/odino/k8s-dev">odino/k8s-dev</a>,
and I believe I should credit <a href="https://www.linkedin.com/in/hzarka/">Hisham</a> for the original idea
&mdash; this is a script we&rsquo;ve been using (and polishing) since ages.</p>

<p>If you&rsquo;re wondering how does it look on the
terminal, here&rsquo;s an asciicast where tests
are run succesfully (<code>dev test</code>), we update
the code to make the tests fail and then
we jump into the container (<code>dev in</code>), before
cleaning up (<code>dev delete</code>):</p>

<script id="asciicast-07QLv6MRWkgurAu7uHxYfA421" src="https://asciinema.org/a/07QLv6MRWkgurAu7uHxYfA421.js" async></script>


<h2>That&rsquo;s a wrap</h2>

<p>Oh boy, right on time to close 2019 with a splash!</p>

<p>Developing on a local k8s cluster isn&rsquo;t super straightforward,
and I hope that by sharing my toolbox it should be easier for
you to set your environment up for a productive day.</p>

<p>Happy new decade!</p>
]]></content>
  </entry>
  
</feed>
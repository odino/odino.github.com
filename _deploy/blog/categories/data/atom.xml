<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: data | Alessandro Nadalin]]></title>
  <link href="http://odino.org/blog/categories/data/atom.xml" rel="self"/>
  <link href="http://odino.org/"/>
  <updated>2013-09-01T18:03:28-07:00</updated>
  <id>http://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Quality isn't always better than quantity]]></title>
    <link href="http://odino.org/quality-isnt-always-better-than-quality/"/>
    <updated>2012-01-25T13:43:00-08:00</updated>
    <id>http://odino.org/quality-isnt-always-better-than-quality</id>
    <content type="html"><![CDATA[<p>Reading about <a href="http://www.amazon.com/Data-Intensive-Processing-MapReduce-Synthesis-Technologies/dp/1608453421">data processing with MapReduce</a>
I was astonished when I first encountered the <em><a href="http://books.google.it/books?id=GxFYuVZHG60C&amp;pg=PA134&amp;lpg=PA134&amp;dq=stupid+backoff+algorithm&amp;source=bl&amp;ots=fMzZNlaNaN&amp;sig=mcEdim6-_wZL4aWKebh3s79KMS4&amp;hl=it&amp;sa=X&amp;ei=z_kfT56BG-vP4QSY2N2ODw&amp;ved=0CD0Q6AEwAzgK#v=onepage&amp;q=stupid%20backoff%20algorithm&amp;f=false">Stupid backoff</a></em>
algorithm&rsquo;s tale.</p>

<!-- more -->


<p>The story is pretty simple: the <a href="diom.ucsd.edu/~rlevy/lign256/winter2008/kneser_ney_mini_example.pdf">Kneser-Ney</a>
smoothing strategy was a <em>state-of-the-art</em> way for processing data, but it
had an heavy computational cost.</p>

<p><blockquote><p>We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.</p><footer><strong>Thorsten Brants <a href="http://acl.ldc.upenn.edu/D/D07/D07-1090.pdf">http://acl.ldc.upenn.edu/D/D07/D07-1090.pdf</a> Large Language Models in Machine Translation</strong></footer></blockquote></p>

<p>In 2007 <a href="http://www.coli.uni-saarland.de/~thorsten/">Thorsten Brants</a> developed
a new smoothing algorithm, simpler than the Kneser-Ney one, which was very lean
and appliable to large amounts of data.</p>

<h2>The result?</h2>

<p>These algorithms were heavily used in machine translations, and you can already
figure out what happened: with small datasets the <em>backoff</em> was generating
less-accurate translations but, as the amount of data analized growed, it was
able to extract more valid translations, eventually beating Kneser-Ney&rsquo;s score<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>.</p>

<p>I&rsquo;d like you to read a few notes about the <em>stupid backoff</em>&rsquo;s introductory paper:</p>

<iframe src="http://docs.google.com/viewer?url=http%3A%2F%2Facl.ldc.upenn.edu%2FD%2FD07%2FD07-1090.pdf&embedded=true" width="100%" height="780" style="border: none;"></iframe>


<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>This was possible, in the machine-translation scenario, thanks to the fact that the algorithm could be &ldquo;trained&rdquo; to perform better translations as the dataset grew <a href='#fnref:1' rev='footnote'>â†©</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
</feed>

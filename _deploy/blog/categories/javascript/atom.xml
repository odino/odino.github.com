<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: JavaScript | Alessandro Nadalin]]></title>
  <link href="http://odino.org/blog/categories/javascript/atom.xml" rel="self"/>
  <link href="http://odino.org/"/>
  <updated>2014-02-27T19:02:51-05:00</updated>
  <id>http://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AngularJS is the future. Maybe.]]></title>
    <link href="http://odino.org/angularjs-is-the-future-maybe/"/>
    <updated>2014-02-27T11:54:00-05:00</updated>
    <id>http://odino.org/angularjs-is-the-future-maybe</id>
    <content type="html"><![CDATA[<p>Today I gave my 2nd talk here at the ConFoo in Montreal,
an overview of why Angular could fit your <em>future</em> requirements
quite well.</p>

<!-- more -->


<p>Enjoy!</p>

<p><div class="slideshare" id="__ss_31723051"><iframe src="http://www.slideshare.net/slideshow/embed_code/31723051 " width="850" height="650" frameborder="0" marginwidth="0" marginheight="0" scrolling="no"></iframe></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How to make your JavaScript apps SEO-friendly]]></title>
    <link href="http://odino.org/how-to-make-your-javascript-apps-seo-friendly/"/>
    <updated>2014-01-17T14:15:00-05:00</updated>
    <id>http://odino.org/how-to-make-your-javascript-apps-seo-friendly</id>
    <content type="html"><![CDATA[<p>One of the challenges in moving your application logic
from the backend (Rails, Django or whatsoever) to the
new frontier of JS web frameworks like AngularJS and
EmberJS is how you can make them SEO-friendly, as these
JavaScript applications get sent to the browser by your
webserver as a <code>200 OK</code>, no matter if, once the app boots,
the page that its being represented is not found, or has
some specific metatags, like title and description.</p>

<!-- more -->


<p>This is, for example, how a tipical angular app&rsquo;s <code>HEAD</code>
section of the HTML looks like:</p>

<p>
``` html
<head></p>

<pre><code>&lt;meta charset="utf-8"&gt;
&lt;title&gt;
    {{ pageTitle }}
&lt;/title&gt;
&lt;meta name="keywords" content="{{ metaKeywords }}"&gt;
&lt;meta name="description" content="{{ metaDescription }}"&gt;
&lt;link type="image/x-icon" data-ng-href="https://example.org/favicon.ico" rel="shortcut icon"&gt;
&lt;link rel="stylesheet" href="https://example.org/screen.css"&gt;
</code></pre>

<p></head>
```
</p>

<p>As you see, the page title, which is what SE like Google use
as main text in your indexed pages, is a mere placeholder for
a variable that come after the JS framework has booted and
executed its own logic, eventually leading to SERP results
like this:</p>

<p><img class="center" src="/images/google-result-placeholders.png"></p>

<p>So, how will you make sure that search engines will actually
see the post-processed HTML and not the very first one that
gets sent from your server?</p>

<p>You have at least a couple different solutions that use the
same underlying tecnique.</p>

<h2>Once feared, Cloaking is your only way</h2>

<p>The practice of <a href="http://en.wikipedia.org/wiki/Cloaking">cloaking</a>
has been penalized by search engines for years but turned out to
be <a href="https://developers.google.com/webmasters/ajax-crawling/docs/html-snapshot">endorsed by Google</a>
when you have JS-based apps.</p>

<p>It basically consist in serving to the search engine a different
version of the webpage, already rendered, instead of the one
that you would serve to a normal visitor, which has to run
the JS framework on the browser.</p>

<p>The workflow is very simple: instead of serving, from the webserver,
your traditional app that you would serve to a normal user, in case
of a bot you simply <strong>forward the request to another application</strong>,
which will request the original page, wait for it to render through
an headless browser like <a href="http://phantomjs.org/">PhantomJS</a> and then
<strong>serve back the fully rendered content</strong> to the bot:</p>

<p><img class="center" src="/images/prerender-seo.svg"></p>

<p>This is a very straightforward way to effectively implement SEO in
JS apps, and it can be achieved with a couple tools instead of
having to write the whole thing on your own.</p>

<h2>BromBone</h2>

<p><a href="http://www.brombone.com/">BromBone</a> is a service that crawls your
sitemap, generates a snapshot of the rendered HTML, stores it on
Amazon (presumably <a href="http://aws.amazon.com/s3/">S3</a>) and relieves you
from the pain of setting up the middleware SEO app on your own.</p>

<p>It basically acts as the SEO app seen in the picture, but instead of
rendering pages on the fly it does it by looking at your sitemap: once
the bot hits the webserver, you can then proxy it to the BromBone page
so that it gets the actual response from the server.</p>

<p>Even though the service is <a href="http://www.brombone.com/#pricing">very affordable</a>
relying on the sitemap it&rsquo;s a bit tricky, because, well&hellip;what happens
if you have new pages that are not included in the sitemap?</p>

<p>After bumping into this requirement we, at Namshi, decided to opt for
something else.</p>

<h2>Prerender</h2>

<p><a href="https://prerender.io/">Prerender</a> is both a SaaS and an
<a href="https://github.com/collectiveip/prerender">open source library</a>
that prerendrs pages on the fly using PhantomJS and some other
nice tricks to <a href="https://prerender.io/server#http-headers">serve the correct status codes and HTTP headers</a>.</p>

<p>The only disadvantage with rendering on the fly is that the bot will
have to wait a bit longer in order to get the response, and this might
result in a penalization from search engines: the solution is very simple,
as you can simply warm up prerender&rsquo;s cache on your own by hitting the URLs
that you want to cache.
In order to refresh the cache, Prerender lets you do <code>POST</code> requests, so
that:</p>

<ul>
<li>a <code>GET</code> request to <code>http://prerender.example.org/http://example.org/foo.html</code>
will prerender the page on the fly, so that you can cache for future requests
by real bots</li>
<li>a <code>POST</code> request to <code>http://prerender.example.org/http://example.org/foo.html</code>
will refresh the prerendered content</li>
</ul>


<p>Prerender gives you a bit more freedom compared to Brombone but it requires you
to do some manual work, at least if you want to run it on your own servers
without using their solution as a SaaS; in any case, their pricing modes
is <a href="https://prerender.io/pricing">very affordable</a> as well.</p>

<h2>What shall I use?</h2>

<p>Surround your pullquote like this {" text to be quoted "}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Frontend web development is not as fun as it could be]]></title>
    <link href="http://odino.org/frontend-web-development-is-not-as-fun-as-it-should-be/"/>
    <updated>2013-11-10T19:53:00-05:00</updated>
    <id>http://odino.org/frontend-web-development-is-not-as-fun-as-it-should-be</id>
    <content type="html"><![CDATA[<p>I am writing this post in the middle of revamping
<a href="http://en-ae.namshi.com">Namshi&rsquo;s architecture</a>
with AngularJS, reverse proxies,
SPDY and HTTP APIs, because I
strongly believe in the future of these technologies and
that they are the de-facto cutting-edge solution for
most of the antipatterns that we&rsquo;ve seen so far:
monolithic applications, unscalable frontends,
limited protocols.</p>

<!-- more -->


<p><img class="right" src="/images/browsers.jpg"></p>

<p>So why would I rant about it? Well, this is not a real
rant but it&rsquo;s more of a retrospective on the <em>gotchas</em>
that we faced over the past months: I do <strong>really</strong>
enjoy all of this techs, but also
recognize that most of them are at a
very early stage and have their pitfalls when it comes
to develop real-world, scalable architectures.</p>

<p>The boring part&rsquo;s over, let&rsquo;s get into the real mess ;&ndash;)</p>

<h2>Reducing redirects?</h2>

<p>Suppose that you have a frontend, maybe built with AngularJS,
that proxies all the requests to an API, so if you request
<code>example.org/about</code>, your frontend actually gets the content
from <code>api.example.org/about</code>.</p>

<p>One of the things that you can start optimizing are the
round trips between the client and the server (very important
for mobile connections): for example,
instead of sending simple redirects from your API to the
frontend, you can return a <code>30X</code> and include the actual body
in the response; in this way, the client can:</p>

<ul>
<li>read the body of the response and work with it (output or whatever)</li>
<li>update the browser URL according to the <code>Location</code> header provided in the response with the <a href="http://diveintohtml5.info/history.html">browsers' history API</a></li>
</ul>


<p>NOT. SO. FAST.</p>

<p>Turns out that modern browsers intercept redirects and make an
additional HTTP request to the <code>Location</code> provided by the response.</p>

<p>This behavior is pretty useful in 98% of your use-cases, as
you dont have to take care of handling AJAX redirects on
your own and you have a pretty simple solution, using a
custom HTTP status
code, like <a href="http://stackoverflow.com/questions/199099/how-to-manage-a-redirect-request-after-a-jquery-ajax-call">278</a>, for the remaining 2% of scenarios.</p>

<p>NOT. SO. FAST. 2.</p>

<p>Of course, the magnificent <a href="http://www.zdnet.com/blog/networking/the-number-one-mobile-web-browser-googles-native-android-browser/2091">Android native browser</a>
will mess this up, thinking that <code>278</code> is an error code: so if, for
your HTTP request, you have a callback in case of success and
one in case of an error, the latter will be triggered.</p>

<p>How to deal with this?</p>

<p>Well, we decided to return straight <code>200 Ok</code> codes and include
2 custom headers, <code>X-Location</code> and <code>X-Status-Code</code>, that our
clients will parse to find out if they need to update the
browser&rsquo;s URL.</p>

<p>In pseudo-code:</p>

<p>```
res = http.get(&lsquo;api.example.org?search=BMW&rsquo;)</p>

<p>if (res.status_code === 200 &amp;&amp; res.headers.x-location) {</p>

<pre><code>browser.url = res.headers.x-location
</code></pre>

<p>}
```</p>

<p>In any case, with the growing amount of mobile clients, I think
it might make sense to start thinking of an appropriate process
to handle redirects, within the HTTP protocol, so that browsers
can just follow the spec: in my opinion we could use something
like <code>308 Transparent redirect</code> for this.</p>

<h2>Reverse proxies and HTTP cache</h2>

<p><img class="left" src="/images/varnish-cache.jpg"></p>

<p>Two of the most <a href="http://www.mnot.net/blog/2007/12/12/stale">important directives</a>
that you can use while taking advantage of the HTTP cache
are <code>stale-while-revalidate</code> and <code>stale-if-error</code>:
the former lets you return stale responses
while you revalidate the cache while the latter lets you serve
cached responses if your backend is down (<code>50X</code> errors).</p>

<p>Of course, you will need a reverse proxy in front
of your webserver in order to really take advantage of
these directives: <a href="http://www.squid-cache.org/">Squid</a> natively implements
both of them but, in our case, it was too much of a hassle to setup,
as it&rsquo;s bloated compared to its cousin <a href="https://www.varnish-cache.org/">Varnish</a>,
which doesn&rsquo;t natively implement <code>stale-*</code> directives instead.</p>

<p>Setting up Varnish to support those 2 directives it&rsquo;s a matter
of a few tries anyhow, as you can mimic the (almost) same
behaviors with Varnish&rsquo;s <a href="https://www.varnish-software.com/static/book/Saving_a_request.html#core-grace-mechanisms">grace and saint modes</a>.</p>

<h2>Android&rsquo;s native browser</h2>

<p><img class="right" src="/images/android.jpg"></p>

<p>Android, oh Android!</p>

<p>As we already saw, its native browser doesn&rsquo;t let you play around
with unconventional HTTP status codes<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup> and, on top of that,
<strong>it breaks the HTTP cache</strong>.</p>

<p>If you have a cacheable resource that you retrieve via AJAX,
the first request to retrieve will work, but as soon as you reload
the page and retrieve it a second time, the browser messes things up,
things that the request returned an invalid HTTP status code (<code>0</code>)
and aborts the process.</p>

<p>And yes, it&rsquo;s a <a href="http://opensourcehacker.com/2011/03/20/android-webkit-xhr-status-code-0-and-expires-headers/">known bug</a>.</p>

<h2>Performances on old devices</h2>

<p>And when I say old I mean stuff like the Galaxy S2 or S3, which are not <strong>that</strong> old
to be honest.</p>

<p>Performances are anyhow a huge concern when you start moving the logic into
the clients, as resources might be very limited: let&rsquo;s not forget that the
first generation of Galaxy &ndash; or even the iPhone 4 &ndash; were shipped out with
just 512mb of RAM; think of a JS-heavy app, which sucks up to 40/50mb of RAM:
how would that perform on those devices?</p>

<p>Let me tell you that: it&rsquo;d be very slow, and would even feel slower when CPUs
comes into the mix as &ndash; we&rsquo;ve witnessed it &ndash; to build DOM elements<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup>
it could take up to 4s. Of course, you can optimize it, but a brand new smartphone
wouldn&rsquo;t let you feel such <em>lag</em>: truth is that when you decide to go for a JS app
you need to take into account the time spent in optimizations for old devices,
as you&rsquo;ll surely need to invest on it.</p>

<h2>What a hard time debugging browser events</h2>

<p><img class="left" src="/images/chrome-devtools.png"></p>

<p>If you&rsquo;ve ever dug deep into optimizing browser events (HTML parsing, repainting and so on)
you probably know what I&rsquo;m talking about: the devtools are still at an early stage and
it becomes really tricky to be able to nail issues down or at least to efficiently
reverse engineer them; even though you have a breakdown of every browser event it&rsquo;s
actually pretty difficult to trace events back to their &ldquo;cause&rdquo;<sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup>.</p>

<p>Chrome provides profiles and the timeline which are very useful resources, but you can&rsquo;t
really inspect that much as at a certain point you&rsquo;ll end up with <strong>a lot</strong> of events
like <em>HTML parsing</em> or <em>Function call</em> and only God knows where they exactly came from.</p>

<h2>Persistent sessions and credentials</h2>

<p><img class="right" src="/images/security.jpg"></p>

<p>Authenticating users might be tricky for frontend apps: you don&rsquo;t have the good old
(and heavy) PHP sessions that you can just fill up on you server, but you &ndash; at least
initially &ndash; can try to persist sessions on cookies or <a href="http://diveintohtml5.info/storage.html">localStorage</a>.</p>

<p>But even before thinking of storing sessions you have to deal with authenticating each
of your user from the app: granted that the HTTP basic auth is not good as
<a href="http://en.wikipedia.org/wiki/Basic_access_authentication#Security">it&rsquo;s flawed</a> and that
the <a href="http://en.wikipedia.org/wiki/Digest_access_authentication">digest auth</a> might be too simple,
you should start looking at alternative methods to authenticate and authorize your users,
preferrably using tokens.</p>

<p>As you probably figured out, there is no real standard mechanism of doing this, as some
providers rely on oAuth (both 2-* and 3-legged) and some on OpenID. Even Mozilla, a while ago,
came out with its own protocol, <a href="http://www.mozilla.org/en-US/persona/">Persona</a>, to solve this
problem.</p>

<p>We actually found out another &ldquo;protocol&rdquo; to store and transmit credentials through HTTP
APIs, the <em>Javascript Object Signing and Encryption</em> specification (<a href="https://datatracker.ietf.org/wg/jose/">JOSE</a>).</p>

<h2>A note on AngularJS and the Grunt ecosystem</h2>

<p>A glimpse of our <code>package.json</code>:</p>

<p>``` json
{
  &ldquo;devDependencies&rdquo;: {</p>

<pre><code>"grunt": "~0.4.1",
"grunt-contrib-copy": "~0.4.1",
"grunt-contrib-concat": "~0.3.0",
"grunt-contrib-coffee": "~0.7.0",
"grunt-contrib-uglify": "~0.2.0",
"grunt-contrib-compass": "~0.3.0",
"grunt-contrib-jshint": "~0.6.0",
"grunt-contrib-cssmin": "~0.6.0",
"grunt-contrib-connect": "~0.4.0",
"grunt-contrib-clean": "~0.4.1",
"grunt-contrib-htmlmin": "~0.1.3",
"grunt-contrib-imagemin": "~0.1.4",
"grunt-contrib-watch": "~0.5.3",
"grunt-usemin": "~0.1.11",
"grunt-rev": "~0.1.0",
"grunt-karma": "~0.4.3",
"grunt-open": "~0.2.0",
"matchdep": "~0.1.2",
...
</code></pre>

<p>```</p>

<p>As you see, the bower/node/grunt/angular ecosystem is still very young and,
from certain points of view, very immature (for example, <a href="https://github.com/gruntjs/grunt-contrib-connect/issues/9">SSL support in Grunt</a>
was added just 2 months ago).</p>

<p>So if you want to use these kind of technologies you must accept that, sometimes,
doing an <code>npm install</code> might break something, or that you will need to keep your deps
updated to the latest releases: it&rsquo;s all about <strong>go big or go home</strong>.</p>

<h2>Internet Explorer. As always.</h2>

<p><img class="right" src="/images/ie.png"></p>

<p>Eheh, there could not be a post about <em>web gotchas</em> without mentioning IE:
the lack of support for <a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>
in IE8 and IE9 is actually a real kicker for
efficiently implementing frontend apps that rely on a remote API
(<code>example.org</code> &ndash;> <code>api.example.org</code>), since CORS considers as cross-domain
even a request on a subdomain<sup id='fnref:4'><a href='#fn:4' rel='footnote'>4</a></sup>.</p>

<p>And no, there is no native escape strategy for this: you <strong>must</strong> extend the
<a href="http://www.w3.org/TR/XMLHttpRequest/">XHR</a> in order to make it capable of doing
cross-domain communication through Iframes, a strategy that even Google and Facebook
have implemented in the past; luckily there is some stuff already written for us,
so we can use the good <a href="https://github.com/jpillora/xdomain">xDomain</a> and
include it with the IE conditional comments.</p>

<p>But then, you would think, why not using xDomain for everything, and simply drop
CORS?</p>

<p>Well, there are a few things to consider:</p>

<ul>
<li>even though Google and Facebook are known to be using some trick like xDomain, it&rsquo;s
pretty strange that they havent released anything yet (might be that they don&rsquo;t consider
it a long-term option)</li>
<li>the native Android browser was known to have <a href="https://github.com/jpillora/xdomain/issues/19">issues with this library</a></li>
<li>CORS is a growing standard that has been widely adopted by the community</li>
<li>the <a href="https://github.com/jpillora/xdomain/blob/gh-pages/dist/xdomain.js">code</a> looks kind of cryptic</li>
</ul>


<p>All in all, we didnt feel like using xDomain for everything, as we are just using
it for IE8/9<sup id='fnref:5'><a href='#fn:5' rel='footnote'>5</a></sup>: <a href="https://github.com/jpillora">Jaime</a> did a great job
implementing it but I personally feel that it might be <strong>too much</strong> to just blindly
rely on it for cross-domain communications.</p>

<h2>CORS and HTTP headers</h2>

<p>Deciding to go with CORS it&rsquo;s just half of it, as the other half consists
into actually implementing CORS on both your clients and servers (APIs): it&rsquo;s
worth mentioning that the specification is really strict when it comes to
<strong>send and manipulate custom HTTP headers</strong> (like the <code>X-Location</code> that we
saw at the beginning of this post).</p>

<p>If you wanna send a custom header you will need to specify it in the
<code>Access-Control-Request-Headers</code> header:</p>

<p><code>
Access-Control-Request-Headers: X-My-Custom-Header
</code></p>

<p>and if you want your clients to be able to access some of the response&rsquo;s headers you will
need to declare them as &ldquo;accessible&rdquo;:</p>

<p><code>
Access-Control-Expose-Headers: X-My-Other-Header, X-...
</code></p>

<h2>How to do RUM?</h2>

<p>Another good question (and I don&rsquo;t have an answer so far) is how to measure
performances as felt by the end-user (<a href="http://en.wikipedia.org/wiki/Real_user_monitoring">Real User Monitoring</a>),
since AngularJS loads once and then never triggers server-side measurable events
other than HTTP requests to your APIs: the API&rsquo;s response times cannot be taken
into account as you wouldn&rsquo;t measure the
performance perceived by the end user &ndash; remember, JS frameworks add HTML parsing,
data-binding and so on on top of the cake.</p>

<p>We still have to figure out how we will implement RUM in our apps,
if you have any suggestion it would be highly appreciated!</p>

<h2>All in all&hellip;</h2>

<p>Developing on the frontend is an amazing experience that has some drawbacks, like on
any platform: beware of the specs (like CORS) and gotchas (IE, Android&rsquo;s native browser) and you will
enjoy it to the max!</p>

<hr />

<p><em>P.S. Thanks to <a href="https://github.com/hzarka">HZ</a> and <a href="https://github.com/AdamQuadmon">AdamQuadmon</a>
for passively contributing to this post, sharing with me and the rest of our team this amazing
and tortuous experience</em></p>

<hr />

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>Which might be fair, as even Chrome has some funny behaviors <a href='#fnref:1' rev='footnote'>↩</a></li><li id='fn:2'>Not so many DOM elements! <a href='#fnref:2' rev='footnote'>↩</a></li><li id='fn:3'>Chrome's Devtools "Profiles" are probably the best resource you can use to backtrace stuff <a href='#fnref:3' rev='footnote'>↩</a></li><li id='fn:4'>Which makes sense when you have platforms like Tumblr <a href='#fnref:4' rev='footnote'>↩</a></li><li id='fn:5'>BTW IE11 is out, so there's even more hope <a href='#fnref:5' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Securing your HTTP API with JavaScript Object Signing and Encryption]]></title>
    <link href="http://odino.org/securing-your-http-api-with-javascript-object-signing-and-encryption/"/>
    <updated>2013-06-04T00:31:00-04:00</updated>
    <id>http://odino.org/securing-your-http-api-with-javascript-object-signing-and-encryption</id>
    <content type="html"><![CDATA[<p>One thing that is always difficult, enough to
deserve <a href="http://www.amazon.com/Ajax-Security-Billy-Hoffman/dp/0321491939">its own book</a>,
is to <strong>secure HTTP API</strong> that interact with client-side
applications: today, after a discussion about how to face
the problem in our company, we bumped into the
<a href="http://datatracker.ietf.org/doc/draft-ietf-jose-json-web-signature/?include_text=1">JOSE</a>
&ndash; JavaScript Object Signing and Encryption &ndash;
specification.</p>

<!-- more -->


<p>Basically, the specification defines 4 entities:</p>

<ul>
<li>JWS, <a href="http://tools.ietf.org/html/draft-jones-json-web-signature-04">JSON Web Signature</a>,
a signed representation of data</li>
<li>JWT, <a href="http://self-issued.info/docs/draft-ietf-oauth-json-web-token.html">JSON Web Token</a>,
a representation of data
(it differs from JWS as JWT is not signed)</li>
<li>JWE, <a href="http://self-issued.info/docs/draft-ietf-jose-json-web-encryption.html">JSON Web Encryption</a>,
an encrypted JSON representation of data</li>
<li>JWA, <a href="http://tools.ietf.org/html/draft-ietf-jose-json-web-algorithms-00">JSON Web Algorithms</a>,
a list of safe algorithms to be used with JWS and JWE</li>
</ul>


<p>For the sake of basic knowledge, we will only have a look
at JWS and JWT / JWE now: the specifications about these
entities are quite extensive and not very straightforward,
so for further details you should <strong>really</strong> give them
a look.</p>

<p><img class="right" src="/images/jws.png"></p>

<h2>JWT</h2>

<p>Basically, the token (JWT) is the simplest structure
that you will deal with while implementing JOSE in our
architecture; it is a string representation of some data
base64 encoded (other types of encoding might be applied, but
this is not madatory): the JWT differs from raw base64-encoded
data since it also includes informations about the encoding
itself, in the token&rsquo;s header; by concatenating the base64-encoded
version of the token header and payload (the actual data) you
obtain what the specification calls <strong>signature input</strong>, which will
then be used to create the signature (JWS).</p>

<h2>JWS and JWE</h2>

<p>After the JWT comes the JWS, which is a signed representation
of the JWT; it differs from the token just because of the
signature; on an higher step of the ladder comes the JWE instead,
which lets you encrypt the data in order to achieve an higher security
level: the <a href="http://self-issued.info/docs/draft-ietf-jose-json-web-encryption.html#JWEExamples">examples in the ietf draft</a>
show you how to create JWEs with a pair of private /
public keys.</p>

<h2>Use case: how to authenticate stateless AJAX calls?</h2>

<p><img class="left" src="/images/jsw-auth.png"></p>

<p>One of the needs that you might have is to,
from JavaScript, make authenticated HTTP calls to
one of your webservices: since you don&rsquo;t want to
expose the WS credentials on the JS service (the
credentials would be readable by any client) a good
solution might be to generate a JWS with a private
OpenSSL key in your webservice, store it into a cookie
accessible to the JS service, which would execute
those calls including that cookie<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>, which you can then
verify while authenticating the call.</p>

<p>This workflow is pretty easy to understand, but the actual
implementation is more than tricky, since the
specification is quite abundant &ndash; especially about
encryption algorithms.</p>

<p>In PHP we can use at least 3 libraries: one of them,
<a href="https://github.com/ritou/php-Akita_JOSE">Akita_JOSE</a>,
is pretty old (since the last commit was more than
7 months ago) but is very understandable and quite
easy to use; another one, <a href="https://packagist.org/packages/gree/jose">gree/jose</a>,
has itw own package on packagist and can be easily
installed via composer: from a fast look at the
<a href="https://github.com/gree/jose">source code on GitHub</a> it looks good,
even though it needs the <a href="http://phpseclib.sourceforge.net/">phpsec</a>
library to be able to work<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup>.</p>

<p>The third option, which is the one that <a href="https://github.com/namshi/jose">I built in the last couple of hours</a>,
is <a href="https://packagist.org/packages/namshi/jose">namshi/jose</a>,
which is very, very easy to use<sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup>: it currently only
supports the <a href="https://github.com/namshi/jose/blob/master/src/Namshi/JOSE/Signer/RS256.php">RSA algorithm</a>
with <code>sha256</code> hashing, but I guess that implementing other
algorithms is less than trivial.</p>

<p>For example, let&rsquo;s see how you would generate the JWS
to be stored in a cookie:</p>

<p>``` php Generating a JWS after authentication and storing it into a cookie
&lt;?php</p>

<p>use Namshi\JOSE\JWS;</p>

<p>if ($username == &lsquo;correctUsername&rsquo; &amp;&amp; $pass = &lsquo;ok&rsquo;) {</p>

<pre><code>$user = Db::loadUserByUsername($username);

$jws  = new JWS('RS256');
$jws-&gt;setPayload(array(
    'uid' =&gt; $user-&gt;getid(),
));

$privateKey = openssl_pkey_get_private("file://path/to/private.key");
$jws-&gt;sign($privateKey);
setcookie('identity', $jws-&gt;getTokenString());
</code></pre>

<p>}
```</p>

<p>and then the apps that want to execute authenticated
calls on behalf of the user by using this cookie just need
to include it in these calls; the server will just need
to verify that the JWS in the cookie is valid:</p>

<p>``` php
&lt;?php</p>

<p>use Namshi\JOSE\JWS;</p>

<p>$jws        = JWS::load($_COOKIE[&lsquo;identity&rsquo;]);
$public_key = openssl_pkey_get_public(&ldquo;/path/to/public.key&rdquo;);</p>

<p>if ($jws->verify($public_key)) {</p>

<pre><code>$paylod = $jws-&gt;getPayload();

echo sprintf("Hey, my JS app just did an action authenticated as user #%s", $payload['id']);
</code></pre>

<p>}
```</p>

<p>That&rsquo;s it: far from being a stable library, this is more a
proof of concept that we, an Namshi, would like to see developing
in the next weeks / months.</p>

<p>As always, comments, rants or &ndash; even better &ndash; pull requests are
<strong>more than welcome</strong>!</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>One of the disadvantages of this approach is that it relies on cookies, only available in the HTTP protocol. If you want to use another protocol for you application - a very rare and extreme use case - this wouldn't work for you. <a href='#fnref:1' rev='footnote'>↩</a></li><li id='fn:2'>I honestly never heard of this library before, so I can't really say what it does and why it's needed <a href='#fnref:2' rev='footnote'>↩</a></li><li id='fn:3'>Since I'm not an expert in encryption and security, I would suggest to give it a look and come up with feedbacks <a href='#fnref:3' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimizing loading times with images' lazy loading]]></title>
    <link href="http://odino.org/optimizing-loading-times-with-images-lazy-loading/"/>
    <updated>2012-09-21T07:45:00-04:00</updated>
    <id>http://odino.org/optimizing-loading-times-with-images-lazy-loading</id>
    <content type="html"><![CDATA[<p>Sometimes I like to forget that I&rsquo;m
mainly involved in other things and
get my hands dirty with low-level
stuff: last week I wanted to improve
some existing lazy loading code we use
with JavaScript.</p>

<!-- more -->


<p>Let&rsquo;s suppose that we have a page
showing almost 100 images, 500kb
per image, 5MB of a webpage.</p>

<p>It&rsquo;s clearly unpractical to load
all the images at once, since you
would force the use to download
stuff that he would only see scrolling
down with the mouse, so the solution
would be to store all the images' path
in an HTML attribute and trigger the load
just for visible images in the current window.</p>

<p>You&rsquo;d have an HTML like this:</p>

<p>``` html
<html>
  <head></p>

<pre><code>...
</code></pre>

<p>  </head>
  <body></p>

<pre><code>...

...

&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img1.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img2.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img3.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img4.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img5.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img6.jpg" /&gt;
&lt;/div&gt;
...
...
...
</code></pre>

<p>  </body>
</html>
```</p>

<p>and the lazy laoding function looks like:</p>

<p>``` javascript
var lazyLoading = function(){</p>

<pre><code>$('img.lazy-loading').each(function(){            
    var distanceToTop = $(this).offset().top;
    var scroll        = $(window).scrollTop();
    var windowHeight  = $(window).height();
    var isVisible     = distanceToTop - scroll &lt; windowHeight;

    if (isVisible) {
        $(this).attr('src', $(this).attr('id'));
    }
});
</code></pre>

<p>}
```</p>

<p>As you see, we only trigger lazy loading for <strong>visible</strong>
items, which are appearing in the current window:
given the <code>windowHeight</code>, we calculate visibility
based on the difference between the item and
the mouse scroll, so that we can see whether the
product is comprehended in the current window
or not.</p>

<p>To trigger lazy loading you need to listen for
mouse scroll events <strong>and</strong> <code>domready</code> ( if some
images would be visible without scrolling ):</p>

<p>``` javascript
$(document).ready(function() {</p>

<pre><code>lazyLoading();

$(window).scroll(function() {
    lazyLoading();
});
</code></pre>

<p>});
```</p>

<p>We, at <a href="http://www.namshi.com/">Namshi</a>, use the same approach for
<a href="http://www.namshi.com/women-shoes/">catalog listing pages</a>.</p>
]]></content>
  </entry>
  
</feed>

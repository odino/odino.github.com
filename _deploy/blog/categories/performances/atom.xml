<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: performances | Alessandro Nadalin]]></title>
  <link href="http://odino.org/blog/categories/performances/atom.xml" rel="self"/>
  <link href="http://odino.org/"/>
  <updated>2013-06-28T14:45:38+04:00</updated>
  <id>http://odino.org/</id>
  <author>
    <name><![CDATA[Alessandro Nadalin]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Profiling PHP applications from the browser]]></title>
    <link href="http://odino.org/profiling-php-applications-from-the-browser/"/>
    <updated>2013-01-25T22:38:00+04:00</updated>
    <id>http://odino.org/profiling-php-applications-from-the-browser</id>
    <content type="html"><![CDATA[<p>In my <a href="/making-the-orientdb-odm-5-times-faster/">previous post</a> I briefly spoke about
<a href="https://github.com/jokkedk/webgrind">Webgrind</a>,
a web-based profiler for PHP:
now I&rsquo;d like to spend some more time
giving an overview on how to install
and use it, as well as <strong>what to look for
when profiling an application</strong>.</p>

<!-- more -->


<p><img class="right" src="/images/webgrind-call-graph-detail.png"></p>

<h2>Profiling in a few words</h2>

<p>As PHP developers, we are rarely used to
profiling: essentially, most of our
applications are not bound to extensive
CPU usage or insanely huge data-processing operations;
the scope of the language is very clear and
even though we <strong>might</strong> need to profile, once
in a while, it&rsquo;s unlikely that we will end up
having problems like
<a href="/book-review-data-intensive-text-processing-with-mapreduce/">optimizing MapReduce algorhitms</a>.</p>

<p>But sometimes we <strong>do</strong> need to profile,
and this will bring on the table bottlenecks of
your applications: within a session of inspection,
you will likely find optimizations that would lead to
a <code>20/30%</code> faster execution time, by just changing
your backend (PHP) code<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>.</p>

<h2>Why Webgrind</h2>

<p>Among all the available profilers for PHP, I
choose to go with Webgrind for a bunch of reasons:</p>

<ul>
<li>nowadays, I am mostly developing on a Mac<sup id='fnref:2'><a href='#fn:2' rel='footnote'>2</a></sup>, so
<a href="http://kcachegrind.sourceforge.net/html/Home.html">KCacheGrind</a>
wasn&rsquo;t an option</li>
<li>I didn&rsquo;t want to install <a href="https://github.com/facebook/xhprof">XHPROF</a>
as it usually takes a few minutes,
even though is probably the best profiler for PHP: facebook uses it
<strong>in production</strong>, and it&rsquo;s able to generate a lot of reports that
would make you face performance optimizations from various perspectives</li>
<li>Webgrind offers a zero-setup installation</li>
</ul>


<h2>Installation with XDebug</h2>

<p><a href="http://xdebug.org/">XDebug</a> is a must for profiling, as it&rsquo;s
the tool through which we can generate the
<a href="http://valgrind.org/docs/manual/cg-manual.html">Cachegrind</a> files, that are basically reports
on the costs of your application&rsquo;s calls.</p>

<p>To enable XDebug&rsquo;s profiling you will have to
tweak your <code>php.ini</code>&rsquo;s configuration:</p>

<p><code>bash Enabling profiling with XDebug
xdebug.profiler_enable = 1
</code></p>

<p>Beware that profiling each request your application
processes can be an expensive job (pages that
would usually load in 2/3 seconds can take up to
10 seconds), so you should &ndash; instead of enabling
the profiler by default &ndash; activate the <code>enable_trigger</code>
directive, which will make XDebug profile your application
only if a specific <code>GET</code> or <code>POST</code> parameter is specified
within the request:</p>

<p><code>bash Using the XDebug profiler in enable trigger mode
xdebug.profiler_enable = 0
xdebug.profiler_enable_trigger = 1
</code></p>

<p>Dont forget to restart apache once you made the changes:</p>

<p><code>bash
sudo apachectl restart
</code></p>

<p>By visiting your application and specifying a special
<code>GET</code> parameter in the URL, you will run your first
profiled PHP response: supposing that you want to
profile the code that runs <code>http://dev.project.com</code>,
just visit <code>http://dev.project.com?XDEBUG_PROFILE=true</code></p>

<p>Once you&rsquo;re done with the XDebug configuration, it&rsquo;s
time to install Webgrind:</p>

<p>``` bash
cd /path/to/your/home/projects</p>

<p>git clone git://github.com/jokkedk/webgrind.git
```</p>

<p>That&rsquo;s it!</p>

<p>You can now access Webgrind at
<code>127.0.0.1/webgrind</code> or &ndash; if you prefer &ndash;
set up a virtual host for it:</p>

<p>``` bash Setting up the virtualhost at <a href="http://webgrind/">http://webgrind/</a>
<VirtualHost *:80></p>

<pre><code>DocumentRoot "/path/to/your/home/projects/webgrind"

ServerName webgrind

&lt;Directory "/path/to/your/home/projects/webgrind"&gt;
    Options Indexes FollowSymLinks MultiViews
    AllowOverride all
    Order allow,deny
    Allow from all
&lt;/Directory&gt;
</code></pre>

<p></VirtualHost>
```</p>

<p>and have Webgrind running at
<code>http://webgrind/</code>.</p>

<h2>Looking at the results</h2>

<p><img class="right" src="/images/webgrind-select-file.png"></p>

<p>Once your application runs, XDebug will generate
the cachegrind files that Webgrind will analyze:
after each PHP response is served from your application,
you can inspect the results from the Webgrind interface,
by just selecting the first file of the list:
it might take some time for Webgrind to generate the
first report, as cachegrind files can easily size up to
100/200 megabytes (files below <code>~50MB</code> will be read in
10 seconds or so).</p>

<p>When the report is generated, you will see the results: I
strongly recommend to generate a report in <strong>milliseconds</strong>, as
it will give you a direct overview on how much time
a function takes rather than having this value as a percentage
compared to the entire application&rsquo;s run.</p>

<p><img class="center" src="/images/webgrind-expensive-call.png"></p>

<p>If you order results by <code>Total inclusive cost</code>, you will
exactly see which ones are the most expensive functions
of your applications: in the example, you will see that the
<code>Doctrine\ODM\OrientDB\Mapper::hydrate</code> method really
kills the performances of my application (<code>10.6</code> seconds).</p>

<p>Having this kind of report is not really useful, as
usually you need to dig deeper to understand which
exact step is making that function taking all that time:
you can investigate further by clicking on a function,
action that will open the call stack after that function
is called:</p>

<p><img class="center" src="/images/webgrind-call-stack.png"></p>

<p>as you see, the problems, here, lies in
<code>Doctrine\ODM\OrientDB\Mapper::createDocument</code> (<code>6.2</code> seconds)
and <code>Doctrine\ODM\OrientDB\Mapper::findClassMappingInDirectories</code>
(<code>4.3</code> seconds), so there you have the explanation why
<code>Doctrine\ODM\OrientDB\Mapper::hydrate</code> takes more than
10 seconds.</p>

<p>Then, take your time to investigate even further and make the
optimal changes in your application, run it with the
profiler enabled once more and have a look at the results:</p>

<p><img class="center" src="/images/webgrind-after-optimization.png"></p>

<p>As you see, after I tweaked my code,
<code>Doctrine\ODM\OrientDB\Mapper::hydrate</code> is not even the
most expensive function at all (<code>Sharah\Controller::getPartial</code> is),
and the previously performance-killer methods, which
would take <code>~6</code> and <code>~4</code> seconds, are now respectively
taking <code>~1</code> and <code>~0.1</code> seconds.</p>

<h2>The call graph</h2>

<p><img class="right" src="/images/webgrind-call-graph.png"></p>

<p>Another interesting feature of Webgrind<sup id='fnref:3'><a href='#fn:3' rel='footnote'>3</a></sup>
is the ability to generate a <strong>call graph</strong> to visualize
bottlenecks in the application: by having a look
at the graph you will have a top-down overview on
how much execution time (expressed in percentage)
a function will take.</p>

<p>When you look at it, you should question
every step of the graph and ask yourself is that
specific function should really take that amount
of time.</p>

<p>For example:</p>

<ul>
<li>if a controller takes <code>20%</code> of the time to run (called <code>TTR</code> from now on),
it might be that you have a design flaw, as it should be
the most expensive part of your application, calling
the models and rendering the view (which are <strong>included</strong> in
the calculated <code>TTR</code>)</li>
<li>if a model&rsquo;s method is taking <code>60% TTR</code>, there is a bad smell:
how come that just retrieving data <strong>once</strong> is taking
more than half of the <code>TTR</code>?</li>
<li>if bootstrapping the application takes <code>15% TTR</code>, then
it&rsquo;s fine, as that is usually the time a well-abstracted
framework needs to provide you a solid foundation to develop
on top of</li>
</ul>


<p>In the image above, you will see that <code>87%</code>
of the execution time is taken by the controller&rsquo;s
action (which is fine) and then equally
distributed (<code>10/20%</code>) across various other
functions that controller calls.</p>

<h2>Conclusions</h2>

<p>Surround your pullquote like this {" text to be quoted "}</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>Beware that for high-scale applications you should focus on bigger and deeper improvements: see http://odino.org/rest-better-http-cache/ <a href='#fnref:1' rev='footnote'>↩</a></li><li id='fn:2'>Shame on me, I know <a href='#fnref:2' rev='footnote'>↩</a></li><li id='fn:3'>Which is implemented in every profiler I used so far <a href='#fnref:3' rev='footnote'>↩</a></li><li id='fn:4'>If you want to seriously profile your PHP application, go for XHPROF <a href='#fnref:4' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Optimizing loading times with images' lazy loading]]></title>
    <link href="http://odino.org/optimizing-loading-times-with-images-lazy-loading/"/>
    <updated>2012-09-21T07:45:00+04:00</updated>
    <id>http://odino.org/optimizing-loading-times-with-images-lazy-loading</id>
    <content type="html"><![CDATA[<p>Sometimes I like to forget that I&rsquo;m
mainly involved in other things and
get my hands dirty with low-level
stuff: last week I wanted to improve
some existing lazy loading code we use
with JavaScript.</p>

<!-- more -->


<p>Let&rsquo;s suppose that we have a page
showing almost 100 images, 500kb
per image, 5MB of a webpage.</p>

<p>It&rsquo;s clearly unpractical to load
all the images at once, since you
would force the use to download
stuff that he would only see scrolling
down with the mouse, so the solution
would be to store all the images' path
in an HTML attribute and trigger the load
just for visible images in the current window.</p>

<p>You&rsquo;d have an HTML like this:</p>

<p>``` html
<html>
  <head></p>

<pre><code>...
</code></pre>

<p>  </head>
  <body></p>

<pre><code>...

...

&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img1.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img2.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img3.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img4.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img5.jpg" /&gt;
&lt;/div&gt;
&lt;div class="imgContainer" &gt;
    &lt;img class="lazy-loading" id="http://example.com/img6.jpg" /&gt;
&lt;/div&gt;
...
...
...
</code></pre>

<p>  </body>
</html>
```</p>

<p>and the lazy laoding function looks like:</p>

<p>``` javascript
var lazyLoading = function(){</p>

<pre><code>$('img.lazy-loading').each(function(){            
    var distanceToTop = $(this).offset().top;
    var scroll        = $(window).scrollTop();
    var windowHeight  = $(window).height();
    var isVisible     = distanceToTop - scroll &lt; windowHeight;

    if (isVisible) {
        $(this).attr('src', $(this).attr('id'));
    }
});
</code></pre>

<p>}
```</p>

<p>As you see, we only trigger lazy loading for <strong>visible</strong>
items, which are appearing in the current window:
given the <code>windowHeight</code>, we calculate visibility
based on the difference between the item and
the mouse scroll, so that we can see whether the
product is comprehended in the current window
or not.</p>

<p>To trigger lazy loading you need to listen for
mouse scroll events <strong>and</strong> <code>domready</code> ( if some
images would be visible without scrolling ):</p>

<p>``` javascript
$(document).ready(function() {</p>

<pre><code>lazyLoading();

$(window).scroll(function() {
    lazyLoading();
});
</code></pre>

<p>});
```</p>

<p>We, at <a href="http://www.namshi.com/">Namshi</a>, use the same approach for
<a href="http://www.namshi.com/women-shoes/">catalog listing pages</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Search engines are making the web slower]]></title>
    <link href="http://odino.org/search-engines-are-making-the-web-slower/"/>
    <updated>2012-04-07T10:00:00+04:00</updated>
    <id>http://odino.org/search-engines-are-making-the-web-slower</id>
    <content type="html"><![CDATA[<p>Like it or not, pushing the work to the clients is
a tecnique which made the web able to scale the way
it is now: <strong>search engines are making it slower</strong> and
less scalable, as they don&rsquo;t want us to do so.</p>

<!-- more -->


<p>The problem is that JavaScript &ndash; the <em>creepy</em> JavaScript &ndash;
is now recognized as a first-level programming language<sup id='fnref:1'><a href='#fn:1' rel='footnote'>1</a></sup>,
although SE are around since more than a decade: thus,
crawlers and spiders, although able to interpretate basic
JS code, cannot do more complex stuff, like managing
<a href="http://handlebarsjs.com/">Handlerbars</a> or <a href="http://mnot.github.com/hinclude/">HInclude</a>.</p>

<p>Or, at least, we don&rsquo;t know if they can.</p>

<p>There would be a workaround to this kind of issue, by just
<strong>serving different content for JS-aware clients</strong>, so that a
spider could see the whole resource without the need of
executing JS code: a workaround that would cost in terms of
development time, but still an acceptable workaround.</p>

<p>The problem, here, is that tis tecnique, known as <a href="http://en.wikipedia.org/wiki/Cloaking">cloacking</a>
is part of the <a href="http://en.wikipedia.org/wiki/Search_engine_optimization#White_hat_versus_black_hat">black hat SEO</a> list, so you basically can&rsquo;t take
advantage of it as malicious web developers would use
cloacking to serve keyword-stuffed contents to bots and
&ldquo;normal&rdquo; webpages to humans, and this is something you
really want to avoid, since SERPs' relevance is an
important part of a user&rsquo;s eb experience.</p>

<p>But, at least, we don&rsquo;t know how search engines would react
to the workaround I just explained.</p>

<h2>What do we need?</h2>

<p>We should have clarifications from SE vendors, to know whether
they are able or not to let us take advantage of great JS-based
technologies able to make our applications scale better, or &ndash; better &ndash;
have fully JS-aware spiders and crawlers, able to elaborate
resources like real-world browsers.</p>

<p>It&rsquo;s not about me, it&rsquo;s not about you, it&rsquo;s about the web: a faster,
and <strong>definitely better</strong>, web.</p>

<p><div class="footnotes">
<span>
Notes
</span>
	<ol>
		<li id='fn:1'>Mainly because of the NodeJS hype <a href='#fnref:1' rev='footnote'>↩</a></li>
	</ol>
</div>
</p>
]]></content>
  </entry>
  
</feed>
